<!doctype linuxdoc system>
<article>
<!-- Initially generated from The_LDP_HOWTO_Generator V0.51 -->
<title>Kadeploy v2
<author>Julien Leduc (<url url="mailto:julien.leduc@imag.fr" name="julien.leduc@imag.fr">), 
Gaetan Peaquin, 
Johann Peyrard (<url url="mailto:jpeyrard@imag.fr" name="jpeyrard@imag.fr">)
<date>V0.02  2005-09-16
<!-- Primary category: 2.3. {Clustering} -->
<!-- Keywords: cluster,  deployment, kadeploy, perl, mysql, on demand, computing environment -->
<!-- Oneliner: Explains howto use kadeploy deploying system -->

<abstract>
<nidx>kadeploy</nidx>
The purpose of this HOWTO is to provide all the necessary steps to install and use kadeploy on demand deploying system in its second version.</abstract>
<toc>  <!-- generate a table of contents here -->
<!-- Space inserted for revision history (using RCS etc.)

     End of revision history -->

<sect>Introduction
<p>
<nidx>kadeploy!introduction</nidx>
Deploying multiple computing environment on clusters or grids cannot be easily handled with current tools that are often designed to deploy a single system on each node. An extension to this system is to allow the deployment of multiple computing environment on every node, without compromising the original system/boot sector.

<sect1>Copyright
<p>
Copyright &copy; 2004-07-20 by Julien Leduc, Gaetan Peaquin.
You are free:
<itemize><item>to copy, distribute, display, and perform the work
<item>to make derivative works
<item>to make commercial use of the work
</itemize>Under the following conditions:
 Attribution. You must give the original author credit.
 Share Alike. If you alter, transform, or build upon this work, you may distribute the resulting work only under a license identical to this one.
<itemize><item>For any reuse or distribution, you must make clear to others the license terms of this work.
<item>Any of these conditions can be waived if you get permission from the author.</itemize>


<sect1>Disclaimer
<p>
Use the information in this document at your own risk. I disavow any potential liability for the contents of this document. Use of the concepts, examples, and/or other content of this document is entirely at your own risk.

All copyrights are owned by their owners, unless specifically noted otherwise. Use of a term in this document should not be regarded as affecting the validity of any trademark or service mark.

Naming of particular products or brands should not be seen as endorsements.

You are strongly recommended to take a backup of your system before major installation and backups at regular intervals.


<!-- =========================== -->
<sect1>News
<p>



<!-- =========================== -->
<sect1>Credits
<p>



<!-- =========================== -->
<sect1>Translations
<p>




<!-- =========================== -->
<sect>Overview
<p>
Kadeploy is a deployment system suite written in perl/shell/c.
It use a mysql databases for nodes information storing, and 
dhcp/tftp for pxe booting.
The package include kadeploy scripts, mysql schema, 
tools for pxe, deployment kernel (linux 2.4).


<sect1>Why use a database ?
<p>
kadeploy uses a database to maintain the persistance of the information about the cluster composition and current state.
<p>
The cluster composition is described via :
<itemize>
  <item> the nodes and their features (name, mac address and ip address)
  <item> the type of hard disk drive (size and device name) and their partitions (size and number)
  <item> the environments available (registered) for deployment with all the relative needed information (path to image, kernel path, etc.)
</itemize>
<p>
The cluster state is described via :
<itemize>
  <item> a snapshot of the cluster state that describes for each partition of each disk and on each node, the partition state, the environment installed on (if any), the number of the last deployment done on it and a brief error description about this deployment (if any)
  <item> the history of the ordered deployment with their start and end dates and their state
</itemize>
<!-- =========================== -->
<sect1>How does the database evolve during a deployment execution ?
<p>
During a deployment execution, its state and the one of the partition involved changes in the database.
<p>
A deployment can be in one of the following states :
<itemize>
  <item> waiting (for the nodes)
  <item> deploying
  <item> terminated
  <item> error
</itemize>
<p>
By convention, there cannot be more than one deployment waiting for nodes. 
<p>
A partition can be in one of the following states :
<itemize>
  <item> to deploy
  <item> deploying
  <item> rebooting
  <item> deployed
  <item> error
</itemize>
<p>
By convention, a node involved in a running deployment cannot take part in another deployment.
<p> 
The two conventions above guarantee that a node cannot be involved in two different deployments at the same time even on different disks and/or partitions.
<p>
When a deployment ends :
<itemize>
  <item> if it has failed on (at least) one node, it is changed to error state. When possible, a brief description of what (or when it) happened is associated to the matching partition ; the ones for which the deployment ended successfully are switched to the 'deployed' state. Thus, during a same deployment, two failures that would occur on two different partitions for different reasons would have different error messages. This enables to know if a failure is due to a global problem or to different local ones.
  <item> on the contrary, if the deployment ended successfully on all the nodes, it is changed to the 'terminated' state. The nodes are ready to be used on the new and freshly deployed environment.
</itemize>




<!-- =========================== -->
<sect1><label id="TFTP_structure"><tt>TFTP</tt> structure
<p>
Here is an example of a <tt>TFTP</tt> structure:
<verb>
/tftpboot/
   PXEClient/
      images_grub/
      pxelinux.cfg/
      pxelinux.0
      messages
      help.txt
   X86PC/
      linux/
         images_grub/
	 pxelinux.cfg/
	 linux.0
	 nbplinux.0
	 messages
	 help.txt
</verb>

This directory structure and allow to respond to different <tt>PXE</tt> request schemes. 
The slight differences are not our business here, you should refer to the different <tt>PXE</tt> standards for this part. 
Let's have a short look at this structure :
<itemize>
   <item><verb>/tftpboot/</verb> is <tt>TFTP</tt> root directory ; it is the directory served by the tftp server
   <item><verb>/tftpboot/PXEClient/</verb> is the tftp directory for non Intel NICs (set by <ref id="tftp_repository" name="tftp_repository"> in the configuration file)
   <item><verb>/tftpboot/X86PC/linux/</verb> is the tftp directory for Intel NICs (set by <ref id="tftp_repository_intel" name="tftp_repository_intel"> in the configuration file)
</itemize>
These two last directories contain both about the same structure :
<itemize>
   <item><verb>images_grub/</verb> that will contain <em>kadeploy</em> generated grub bootloaders, to allow the nodes to boot on the deployed system image (set by <ref id="tftp_relative_path" name="tftp_relative_path"> in the configuration file)
   <item><verb>pxelinux.cfg/</verb> that links every node to its kernel/initrd pair to load and boot from network, an ensure that we can remotely control the way the nodes are booting (set by <ref id="pxe_rep" name="pxe_rep"> and <ref id="pxe_rep_intel" name="pxe_rep_intel"> in the configuration file)
</itemize>

These four directories (two in each tftp directory) should be writtable by the <tt>deploy</tt> user, to allow <em>kadeploy</em> to control the way the nodes are booting. 














<!-- =========================== -->
<sect>Installation
<p>
The goal of this part is to ease the deployment system installation steps and give information about all the relevant required tools and their configuration.

<!-- =========================== -->
<sect1>Network configuration
<p>
This Howto will suppose you are building a cluster on a private Ip network.
The cluster nodes have Ip from 192.168.0.1 to 192.168.0.250.
The gateway, dns, dhcp, tftp, mysql services are located on the node 192.168.0.254.


<!-- =========================== -->
<sect1>Perl
<p>
A decent version of Perl need to be installed on the box. 
Actually the 5.8 works fine.
<!-- =========================== -->
<sect1>Mysql
<p>
Kadeploy has been tested with mysql 4.1.

<sect2>How to i create the database ?
<p>
It is possible to create both databases manually. 
All you have to do is to run the command lines below. 
This will create the deploy and deploy_right databases and the appropriate 
users with read and/or write rights.

<p><tt># mysql -u root -p < share/mysql//deploy_db_creation.sql</tt>
<p><tt>Enter password: ********</tt>
<p><tt># mysql -u root -p < share/mysql/rights_db_creation.sql</tt>
<p><tt>Enter password: ********</tt>
<p><tt># mysql -u root -p < share/mysql/patch_dbdeploy_fdisktype.pl</tt>
<p>


Next step will be to fill in the database. That can be done easily using the kaaddnode command.
<!-- =========================== -->
<sect2>How to configure access to databse ?
<p>
In order for the deployment system to access to the database, correct information e.g. database name, login and password should be filled in the configuration file for each database. See the <ref id="database" name="database"> part in the configuration file section for further details.



<!-- =========================== -->
<sect1><tt>PXE/TFTP</tt>
<p>
This part explains how to configure <tt>PXE/TFTP</tt> to control the booting process on the cluster nodes.
<!-- =========================== -->
<sect2>Hardware specifications
<p>
The nodes' hardware must be <tt>PXE</tt> compliant, this means that the <tt>BIOS/EFI</tt> should allow to boot from the ethernet card. To ensure that, you must verify that the network interface is <tt>PXE</tt> compliant.
<!-- =========================== -->


<sect2>Required software
<p>
To be able to handle <tt>PXE</tt> requests from the booting nodes, 
the server should have a set of running servers, which must 
fit minimum requirements:
<itemize>
  <item><tt>DHCP</tt> server, the easiest is to use the one developped by <url url="http://www.isc.org/index.pl?/sw/dhcp/" name="ISC"> in its third version. It is available in many distributions:
	<itemize>
		<item>Debian <em>dhcp3-server</em>
		<item>Mandrake <em>dhcp-server</em> and you should ensure the 
                package release is at least 3.0
  	</itemize>
  <item>Inetd/Xinetd super server a daemon launcher use by many linux distribution.
  <item>TFTP server, particulary <em>tftp-hpa</em> server also available under package :
      	<itemize>
	       	<item>Debian <em>tftpd-hpa</em> 
                <item>Mandrake <em>tftp-server</em> rpm in a minimum 0.29-1mdk version is required to take <url url="http://rpmfind.net/linux/RPM/mandrake/9.2/i586/Mandrake/RPMS/tftp-server-0.34-1mdk.i586.html" name="hpa support"> into account
	</itemize>
   <item><tt>SYSLINUX</tt> include in kadeploy package <tt>libboot/pxelinux/</tt>.
It contains a lot of booloader. 
Pxelinux.0 (pxe bootloader), Memdisk (floppy/hard drive image loader ). 
In our case we use <tt>pxelinux.0</tt> from <url url="http://syslinux.zytor.com" name="syslinux">. 
This file is downloaded by the client, then launch, finally it download is conf from the pxelinux.cfg 
directory (see the syslinux doc for more).
</itemize>



<!-- =========================== -->
<sect2><label id="dhcpd.conf"> <tt>dhcpd.conf</tt> example
<p>
An example of server configuration.
<verb>
allow booting;
allow bootp;
deny unknown-clients;

option domain-name "mycluster.net";
option domain-name-servers 192.168.0.254;

option subnet-mask 255.255.255.0;
default-lease-time 600;
max-lease-time 7200;

subnet 192.168.0.0 netmask 255.255.255.0 {
  range 192.168.0.0 192.168.0.250;
  option broadcast-address 192.168.0.255;
  option routers 192.168.0.254;
}


host cls1 {
  hardware ethernet 00:01:02:04:73:da;
  fixed-address 192.168.0.1;
  filename "pxelinux.0";
  server-name "cls1";
}

host cls2 {
  hardware ethernet 00:01:02:02:a7:f5;
  fixed-address 192.168.0.2;
  filename "pxelinux.0";
  server-name "cls2";
}
</verb>

<!-- =========================== -->
<sect2><label id="inetd.conf"> <tt>inetd.conf</tt> example
<p>
An example of inetd.conf
<verb>
tftp            dgram   udp     wait    nobody /usr/sbin/tcpd /usr/sbin/in.tftpd --tftpd-timeout 300 --retry-timeout 5 --maxthread 100 --verbose=5  /tftpboot
</verb>




<!-- =========================== -->
<sect1>configure && make && makeinstall
<p>
<em>kadeploy</em> have to be install somewhere on your system.
<itemize>
 <item>This can be done by a simple <tt>./configure --prefix=/usr/local && make && make install
 <item>The bin sbin content need to be accessible from the path (symlink or correct your prefix).
 <item>A line like this one must be in your <tt>/etc/kadeploy/deploy.conf</tt>
  <verb>kadeploy2_directory = /my_path_to_kadeploy_directory/</verb>
 <item>kadeploy need some lib perl, "pm" files.
This file are located in <tt>cmd/libkadeploy2/</tt> directory in the package.
You can symlink it in <tt>/usr/lib/perl/5.8/</tt> (or a directory included in your local Perl @INC).
</itemize>


<sect1>Configuration files.
<p>
It is mandatory to create a <tt>/etc/kadeploy</tt> directory, and put
the two configuration file in it.
<itemize>
 <item><tt>/etc/kadeploy/deploy.conf</tt> contain tools and directory path, 
global variable, database information, and kernel boot parameters.
 <item><tt>/etc/kadeploy/deploy_cmd.conf</tt> contain the command to be exectuted for each node.
</itemize>
Example of each file can be found in <tt>tools/cookbook/conf/</tt>.


<!-- =========================== -->
<sect1>System Requirements
<p>
<!-- =========================== -->
<sect2>Required privileges to execute <em>kadeploy</em>
<p>
<em>kadeploy</em> requires to perform some changes on the server, to control the way the nodes are booting. Some other tools like <em>kaconsole</em> or <em>kareboot</em> needs to connect to other computers. The purpose of this part is to define all the required privileges to ensure a smooth execution of <em>kadeploy</em>, and secure it by avoiding unnecessary privileges.

<!-- =========================== -->
<sect>Using kadeploy v2
<p>
The deployment system is composed of several tools or commands whose aim is the execution of all the needed actions to carry out deployments and other administrative tasks.
<p>
Every tool has his own <ref id="Man_pages" name="man page"> available at the end of this documentation. This could be useful to answer some pertinent questions the reader may wonder.
<!-- =========================== -->
<sect1>The first time...
<p>
The two first steps are to register the cluster "software and hardware composition" in its current state. 
<p>
If any environment is already installed on some partitions of some nodes, it is necessary to register it. This operation is done by giving the appropriate information (image name, image location, kernel path etc.) to the <ref id="karecordenv" name="karecordenv"> tool. 
<p>
Then, <ref id="kaaddnode" name="kaaddnode"> is used to register the cluster "hardware" composition. It needs the description of hosts, disks and partitions in a text file. The tool reads it and registers the information (name and addresses of nodes, disk type and size, partition number and size etc.) in the database.
<p>
Once these operations completed, the system is ready for deployments.


<p>
Example of <tt>cluster_description.txt</tt> file.
 <verb>
# Nodes description
cls1        00:01:02:04:73:da     192.168.0.1

\#

# Partition description

hda 8000
1   500  swap
2   3000 empty
3   500  tmp
5   2000 empty
6   2000 empty
</verb>

<p>
You need to execute the command:
<verb>
#kaaddnode /etc/kadeploy/cluster_description.txt
Checking variable definition...
Registration completed.
</verb>


<!-- =========================== -->
<sect1>Deployment
<p>
The system has been prepared for deployment. Now what ? To deploy, something to be deployed is needed !
<p>
The environment to be deployed can either already exist and be registered in the database or already exist but not be registered or neither exist nor be registered. The first case is the simplest since nothing has to be done prior to the deployment itself. In the other cases, <ref id="kacreateenv" name="kacreateenv"> (<ref id="kaarchive" name="kaarchive">/<ref id="karecordenv" name="karecordenv">) are used to create and register (create/register) an environment in the database (cf. the environment section for details about these steps).
<p>
The deployment itself i.e. of a given environment on target partitions of a set of cluster nodes is done using the <ref id="kadeploy" name="kadeploy"> tool.
<p>
A complete deployment is composed of the following steps :
<itemize>
  <item>reboot on the deployment kernel via pxe protocol
  <item>pre-installation actions (bench, partitionning and/or file system building if needed etc.)
  <item>environment copy
  <item>post-installation script sending and execution
  <item>reboot on the freshly installed environment
</itemize>
<p>
If the deployment fails on some nodes, these are rebooted on a default environment.
<p>
Please refer to the <ref id="About_the_customization_scripts" name="customization scripts section"> for further details about preinstall and postinstall scripts.
<!-- =========================== -->

<sect1>Preinstall & Postinstall
<p>
The preinstall and postinstall are tarball archive.
They contains many files AND a <tt>main</tt> scripts.
They are pushed on the node, extracted, and the <tt>main</tt> script is launched.
(see <tt>/etc/kadeploy/deploy.conf</tt> for setting tarball and script).
<itemize>
 <item>Preinstall contains actions that are needed in order to deploy the image on the node.
fdisk, mkfs, mkswap, mount, bench...
 <item>Postinstall contains actions that can't be put in the preinstall scripts.
Manage the fstab, the ssh services, and what you think important for a node...
</itemize>
<p>
Actually you have to make yourself your fdisk.txt files...
This file is liked a fdisk, script.
Not hard but boring, it will be fix sooner.


<!-- =========================== -->
<sect1>Cluster BootStrap
<p>
You need a tarball of a linuxbox.
something like <tt>myLinux.tgz</tt> that can be created simply like this:
<verb>
#cd /
#tar czvlf /pathToAnotherFileSystem/myLinux.tgz /
</verb> 
<p>
You now need to record this environement in kadeploy database.
<verb>
#karecordenv 
            -n "myLinux" 
            -v 2 
            -d "Newbie testing" 
            -a noobs@mycluster.net 
            -fb file://home/noobs/myLinux.tgz 
            -ft file://home/noobs/traitement.tgz  
            -s 900 
            -i /boot/initrd.img-2.6.8-1-686-smp 
            -k /boot/vmlinuz-2.6.8-1-686-smp
</verb>

<p>
Now you have got, an environnement, you have to watch for a preinstall, postinstall archive.
An example is given in <tt>kadeploy2/tools/cookbook/</tt>.

<p>
The cluster need a reference environnement to boot up properly.
It's a chicken eggs problem.
A solution, is 
<itemize>
<item>Set the nodes in deployement state with the <tt>pxe_setup.pl</tt> tools (grep label in deploy.conf).
 <verb>#setup_pxe.pl 129.88.69.111:label_deploy_ecluster_hda2 </verb>
<item>Test the communication with <tt>rsh</tt>.
 <verb>#rsh nodeName /bin/ash </verb>
<item>fdisk the node.
 <verb>#fdisk /dev/hda </verb>
<item>mkfs the node.
 <verb>#mkfs /dev/hda1 </verb>
<item>mount the reference partition.
 <verb>#mkdir /mnt/hda1 && mount /dev/hda1 /mnt/hda1 </verb>
<item>push a reference image on it.
 <verb>rsh nodeName "cd /mnt/hda1 ; tar xzvf -" < /pathTo/myLinux.tgz </verb>
<item>umount it.
 <verb>#rsh nodeName /sbin/umount /mnt/hda1 </verb>
<item>Set the node in reference state with <tt>pxe_setup.pl</tt>.
 <verb>#setup_pxe.pl 129.88.69.111:label_deploy_ecluster_ref </verb>
<item>reboot the node.
 <verb>#rsh nodeName /sbin/reboot </verb>
</itemize>

<p>
Now you can ssh the node, and kadeploy a new environnement.
If you want to simplify your life, set ssh in public key authentification, it will be
much simplier...



<!-- =========================== -->
<sect1>Other tools for remote management
<p>
Two remote management tools are available to diagnostic and if possible take control of cluster nodes that would be for instance in an unstable or undefined state further to a deployment failure. 
<p>
The <ref id="kaconsole" name="kaconsole"> tool enables to open a console on a remote node. It needs special hardware equipment and special command configuration as described in the second part of the configuartion files section.
<p>
The <ref id="kareboot" name="kareboot"> tool enables to do various reboot types on cluster nodes. The possibilities are numerous : the reboot on a given (already installed) environment or on a given partition or even on the deployment kernel for instance. It also enables to hard reboot nodes appropriately equiped.
<!-- =========================== -->
<sect1>What if the cluster hardware composition changes ?
<p>
<ref id="kaaddnode" name="kaaddnode"> and <ref id="kadelnode" name="kadelnode"> enable to add and remove nodes from the deployment system if the cluster composition changes.
<!-- =========================== -->
<sect1>Command summary 
<p>
<sect2>Functionalities summary
<p><tt>kaaddnode</tt> - registers nodes in deployment system
<p><tt>kadelnode</tt> - unregisters nodes in deployment system
<p><tt>karecordenv</tt> - registers an environment image in deployment system
<p><tt>kaarchive</tt> - creates an environment image
<p><tt>kacreateenv</tt> - creates and registers an environment image in deployment system
<p><tt>kadeploy</tt> - deploys an environment image
<p><tt>kaconsole</tt> - opens a remote console 
<p><tt>kareboot</tt> - reboots according to requested reboot type
<p>
<sect2>Use summary
<p>The first time...
<itemize>
  <item<tt>karecordenv</tt> - to register environments already installed (if any)
  <item><tt>kaaddnode</tt> - to register the cluster hardware composition
</itemize>
<p>Deployment
<itemize>
  <item><tt>kacreateenv/karecordenv/kaarchive</tt> - to create and/or register an environment and make it available for deployment
  <item><tt>kadeploy</tt> - to deploy a registered environment
</itemize>
<p>Other tools
<itemize>
  <item><tt>kaconsole</tt> - to open a console on a remote node
  <item><tt>kareboot</tt> - to reboot a cluster node
</itemize>
<sect2>Examples
<p>
<verb>
# karecordenv -n debian -d "custom debian" -fb file://home/toto/images/custom_debian.tgz -ft file://home/toto/images/debian_postinstall.tgz -size 750 -k /boot/vmlinuz 
# kaaddnode cluster_description.txt
# kacreateenv -e new_debian -fb file://home/toto/images/new_debian.tgz -ft file://home/toto/images/debian_postinstall.tgz --size 650 -k /boot/vmlinuz --root-directory /
# kadeploy -e new_debian -m node1 -m node2 -p hda7
# kaconsole -m node2
# kareboot -s -e custom_debian -m node2
</verb>

<!-- =========================== -->
<sect>About the customization scripts<label id="About_the_customization_scripts">
<p>
Kadeploy allows the customization of each node by 2 means:
<itemize>
  <item>preinstallation script, executed before sending the system image
  <item>postinstallation, executed after having sent the system image
</itemize>
Originally, these two scripts are written in <em>ash</em>, which is a lightweight bash, but the way these scripts are designed could allow to add any script language.

<!-- =========================== -->
<sect1>Preinstallation script
<p>
This script is common to all environments, its goal is to prepare the system to the hardware specification and the target hard disk drive for the deployment. It can load a specific IDE controller driver, improve deployment performance or make every kind of checks needed. This script is defined in the configuration file as <ref id="pre_install_script" name="pre_install_script"> and the associated archive as <ref id="pre_install_archive" name="pre_install_archive">.
<!-- =========================== -->
<sect2>Preinstallation archive structure
<p>
The preinstallation archive is a gzipped tar archive, containing the <ref id="pre_install_script" name="pre_install_script"> in its root directory.
Here is an example of a preinstallation archive structure:
<verb>
/
   init.ash
   lib/
   bin/
      awk
      df
      du
      xargs
</verb>

The directory structure allows to custom the tasks to your needs. In this example, the pre_install_script is <em>init.ash</em>. Let's have a short look at this structure :
<itemize>
   <item><verb>init.ash</verb> is my pre_install_script, so it needs to be there
   <item><verb>bin/</verb> is a directory where you can add custom binaries, here, I needed <em>awk</em>, <em>df</em>, <em>du</em> and <em>xargs</em>.
   <item><verb>lib/</verb> is a directory where you can add custom libraries for your binaries. I suggest you to compile static binaries, to prevent conflicts/version problems due to the presence of the system's shared libraries.
</itemize>

<!-- =========================== -->
<sect1>Postinstallation script
<p>
This script is associated to the environment to deploy. Its goal is to adapt the crude system image to a bootable system. It is composed of a gunzipped tar archive that contains all the sites files and a script <em>traitement.ash</em> in the archive's root directory. This archive is sent to the nodes, decompressed in a ramdisk and then the <tt>post_install_script</tt> is executed on every node. The script name is defined in the configuration file as <ref id="post_install_script" name="post_install_script">.
<!-- =========================== -->
<sect2>Postinstallation archive structure
<p>
The postinstallation archive is a gzipped tar archive, containing the <ref id="post_install_script" name="post_install_script"> in its root directory.
Here is an example of a postinstallation archive structure:
<verb>
/
   traitement.ash
   etc/
      fstab
      hosts
      hosts.allow
      hosts.deny
      ntpdate
   authorized_keys
</verb>

The directory structure allows to custom the configuration script to your needs. Let's have a short look at this structure :
<itemize>
   <item><verb>traitement.ash</verb> is my post_install_script.
   <item><verb>etc/</verb> is a directory where I decided to put all the files I wanted to replace on my system, this is an arbitrary choice, but allows to keep a clean structure.
   <item><verb>authorized_keys</verb> is the root's authorized_keys file I decided to put it in the root directory to be sure to have a look at it everytime I update my postinstallation archive.
</itemize>

<!-- =========================== -->
<sect2>System modifications
<p>
<enum>
   <item><verb>/etc/fstab</verb> a site base file should be copied in the postinstall archive so that nfs mounts can be preserved and other site modifications could be preserved
   <item><verb>/tmp</verb> should have its rights modified
</enum>
<!-- =========================== -->
<sect2>Administrative aspects
<p>
Many other basic files can be handled by putting them in the archive and replace the existing ones. They are not all listed here but only the most important ones :
<enum>
   <item><verb>/root/.ssh/authorized_keys</verb> should contain the administrator's public key and also the public key of user <tt>deploy</tt>, to allow him to get a root shell on every node to reboot those. In order to do that this authorized_keys file has to be built and put in the archive's root directory
   <item><verb>/etc/hosts /etc/hosts.allow /etc/hosts.deny</verb> should be set to fit the cluster's configuration, and ensure network connection within the cluster's network
</enum>

Numerous modifications can be done here, from authentification server to tailored modification depending on the node's IP. A good idea should be to modify <tt>rc</tt> scripts to prevent the first boot hard disk drive verification, because it is just a waste of time here, and avoid all the manual intervention that could occur on system boot : for example, by default, many distributions ask the root password before checking a filesystem on boot time.


<!-- =========================== -->
<sect>Configuration Files
<p>
<em>kadeploy</em> tools suite is configured through two configuration files : <tt>deploy.conf</tt> and <tt>deploy_cmd.conf</tt> in the <tt>/etc/kadeploy/</tt> folder.
<!-- =========================== -->
<sect1>deploy.conf
<p>
This file presents all the variables the user must defined in order to be able to use the deployment system. It is divided into several sections. A very short description is available for each variable and sometimes example values are given in commentary.
<p>
The user can find an example in the tools/cookbook folder.
<!-- =========================== -->
<sect2>nmap
<p>
<descrip>
 <label id="enable_nmap"><tag>enable_nmap</tag> value is 0 or 1, setting it to 1 enables ping checks before <em>rsh</em> or <em>ssh</em> checks, to fasten checks on the arrival of the nodes
 <label id="nmap_cmd"><tag>nmap_cmd</tag> value is the absolute path of the nmap command
</descrip>
<!-- =========================== -->
<sect2>sentinelle
<p>
<em>sentinelle</em> is a parallel launcher that is used by <em>kadeploy</em> to perform all the parallel tasks required during the deployment process. Two environments are defined : the first is the deploy environment, which should be configured to let the parallel launcher work when the target nodes are running the deployment kernel, and the prod environment, which is used to issue parallel commands when the nodes are rebooted under the deployed system.
<descrip>
 <label id="deploy_sentinelle_cmd"><tag>deploy_sentinelle_cmd</tag> value is the absolute path of the sentinelle command used when the nodes are running the deployment kernel, this should be <tt>/usr/local/bin/DKsentinelle</tt>, and that is where you should copy the <em>DKsentinelle</em> binary
 <label id="deploy_sentinelle_default_args"><tag>deploy_sentinelle_default_args</tag> value is the default arguments given to the <tt>deploy_sentinelle_cmd</tt> to be able to connect to the nodes and timeout
 <label id="deploy_sentinelle_pipelined_args"><tag>deploy_sentinelle_pipelined_args</tag> value is the <tt>deploy_sentinelle_cmd</tt> arguments given during pipelined operations, it is possible to adapt it to ensure that the pipeline topology will meet the throughput constraints and the reliability needs
 <label id="deploy_sentinelle_endings"><tag>deploy_sentinelle_endings</tag> value is the command to issue to get a node's IP when it is running the deployment kernel
 <label id="deploy_sentinelle_timeout"><tag>deploy_sentinelle_timeout</tag> value is the number of seconds to wait for a parallel command to succeed
 <label id="prod_sentinelle_cmd"><tag>prod_sentinelle_cmd</tag> value is the absolute path of the sentinelle command used when the nodes are running the deployed system, this path is common to the server issuing the deployment, and the target nodes
 <label id="prod_sentinelle_default_args"><tag>prod_sentinelle_default_args</tag> value is the default arguments given to the <tt>prod_sentinelle_cmd</tt> to be able to connect to the nodes and timeout
 <label id="prod_sentinelle_pipelined_args"><tag>prod_sentinelle_pipelined_args</tag> value is the <tt>prod_sentinelle_cmd</tt> arguments given during pipelined operations, it is possible to adapt it to ensure that the pipeline topology will meet the throughput constraints and the reliability needs
 <label id="prod_sentinelle_endings"><tag>prod_sentinelle_endings</tag> value is the command to issue to get a node's IP when it is running the deployed system
 <label id="prod_sentinelle_timeout"><tag>prod_sentinelle_timeout</tag> value is the number of seconds to wait for a parallel command to succeed
</descrip>
<!-- =========================== -->
<sect2><label id="database">database
<p>
<descrip>
 <label id="deploy_db_host"><tag>deploy_db_host</tag> value is the hostname where the mysql server that owns the deployment system database is running
 <label id="deploy_db_name"><tag>deploy_db_name</tag> value is the name of the deploy database
 <label id="deploy_db_login"><tag>deploy_db_login</tag> value is the login to access to the deploy database
 <label id="deploy_db_psswd"><tag>deploy_db_psswd</tag> value is the password to access to the deploy database
 <label id="rights_db_host"><tag>rights_db_host</tag> value is the hostname where the mysql server that owns the deployment rights database is running
 <label id="rights_db_name"><tag>rights_db_name</tag> value is the name of the rights database
 <label id="rights_db_login"><tag>rights_db_login</tag> value is the login to access to the rights database
 <label id="rights_db_psswd"><tag>rights_db_psswd</tag> value is the password to access to the rights database
</descrip>
<!-- =========================== -->
<sect2><label id="preinstall_and_postinstall">preinstall and postinstall
<p>
<descrip>
 <label id="pre_install_archive"><tag>pre_install_archive</tag> value is the absolute path to the preinstall archive containing the pre_install_script
 <label id="pre_install_script"><tag>pre_install_script</tag> value is the name of the script in the preinstall archive that is supposed to be launched during the preinstall step 
 <label id="post_install_script"><tag>post_install_script</tag> value is the name of the script in the postinstall archive, defined in the database for each environment, that is supposed to be executed during the postinstall step
</descrip>
<!-- =========================== -->
<sect2><tt>PXE/TFTP</tt>
<p>
This part deals with the configuration of the <tt>PXE/TFTP</tt> structure, to allow <em>kadeploy</em> to control the way the nodes boot.
<descrip>
 <label id="kernel_param"><tag>kernel_param</tag> value is the default kernel parameters given to the deployed kernel on boot, if none is specified with the environment, these parameters often deals with console redirection during kernel booting process
 <label id="tftp_repository_intel"><tag>tftp_repository_intel</tag> value is the base directory in the TFTP structure for Intel NICs (for our <ref id="TFTP_structure" name="example">, value should be <tt>/tftpboot/X86PC/linux/</tt>)
 <label id="pxe_rep_intel"><tag>pxe_rep_intel</tag> value is the relative path from <tt>tftp_repository_intel</tt> to to the folder where kernel/initrd are associated to each node (for our <ref id="TFTP_structure" name="example">, value should be <tt>pxelinux.cfg/</tt>)
 <label id="tftp_repository"><tag>tftp_repository</tag> value is the base directory in the TFTP structure for non Intel NICs (for our <ref id="TFTP_structure" name="example">, value should be <tt>/tftpboot/PXEClient/</tt>)
 <label id="pxe_rep"><tag>pxe_rep</tag> value is the relative path from <tt>tftp_repository</tt> to to the folder where kernel/initrd are associated to each node (for our <ref id="TFTP_structure" name="example">, value should be <tt>pxelinux.cfg/</tt>) 
 <label id="tftp_relative_path"><tag>tftp_relative_path</tag> value is the directory where the <em>grub</em> files are generated (for our <ref id="TFTP_structure" name="example">, value should be <tt>images_grub/</tt>)
</descrip>
Some other custom variables are available here to ease the cluster management, by defining kernel/initrd shortcuts for <em>kareboot</em>. These custom variables begin with <tt>label_</tt> their values are kernel/initrd pairs that can be assigned to a booting node.
<verb>label_deploy_tg3 = duke-vmlinuz:duke-initrd  ETH_DRV=tg3 console=tty0 console=ttyS0,38400n8 ramdisk_size=20000</verb> means that the booting node will load <tt>duke-vmlinuz</tt> and <tt>duke-initrd</tt> from the <verb>/tftpboot/PXEClient/images_grub</verb> directory passing the following parameters to the kernel <verb>ETH_DRV=tg3 console=tty0 console=ttyS0,38400n8 ramdisk_size=20000</verb>

<!-- =========================== -->
<sect1>deploy_cmd.conf
<p>
This file contains the appropriate reboot and remote console opening commands. The kareboot and kaconsole tools get the commands to execute according to node given in parameter from it.
<p>
Also, the user can find an example in the tools/cookbook folder with a brief description of the syntax to use.
<p>
The syntax is <tt>host_name command_type command</tt> where possible command types are softboot, hardboot, deployboot or console.
<p>
<descrip>
 <label id="softboot"><tag>softboot</tag> the sofboot line is the software reboot command. Typically it looks something like <tt>ssh user@hostname /sbin/reboot</tt> 
 <label id="deployboot"><tag>deployboot</tag> the deployboot line is the command to execute to reboot the node under the deployment kernel e.g. using the <tt>setup_pxe.pl</tt> utility in the <tt>boot/</tt> directory
 <label id="hardboot"><tag>hardboot</tag> hardboot is the hardware reboot command depending on special hardware equipment
 <label id="console"><tag>console</tag> console is the command to open a remote console on a cluster node e.g. via serial port using the kermit software
</descrip>

<!-- =========================== -->
<sect>Man pages<label id="Man_pages">
<p>
This section regroups the man pages for the different tools that compose the deployment system.
<!-- =========================== -->
<sect1>kaaddnode<label id="kaaddnode">
<p>
<sect2>NAME
<p>
kaaddnode - registers nodes and their disk and partition features
<p>
<sect2>SYNOPSIS
<p>
<tt>kaaddnode info_file</tt>
<sect2>DESCRIPTION
<p>
kaaddnode registers nodes and their disk and partition features for them to be available to the deployment system. It can be used at the very beginning to describe a whole cluster (after having completed the installation for instance) or later when new nodes are added to a cluster already managed by the deployment system.
<p>
info_file is the name of the file that describes all the information the system needs. It contains two main parts and has to be formatted as described below:

<verb>
# first part : node set description

node_name1    mac_address1    ip_address1
node_name2    mac_address2    ip_address2
node_name3    mac_address3    ip_address3
...           ...             ...
</verb>	
N.B. : each node is described on a separate line; fields on each line are separated by tabs or spaces. 
<verb>
# second part : descriptive information about the type of disk and the partitioning

device_name    device_size
partition_number1    partition_size1    deployed_env1
partition_number2    partition_size2    deployed_env2
partition_number3    partition_size3    deployed_env3
...		     ...		...
</verb>
N.B. : each partition is described on a separate line; fields on each line are separated by tabs or spaces. All size are given in Mbytes. The specified deployed_env should already exist in the deployment system otherwise default value 'undefined' will be switched to the declared one during the execution.
<p>
First and second part are separated by a - strange and arbitrary - line starting with the following characters : \#. Empty lines and commentaries (starting with #) are ignored. Before doing any modification in the deployment system database, kaaddnode tries to check if the expected description syntax is respected in both parts.
<sect2>EXAMPLE
<p>
<tt>kaaddnode description.txt</tt>
<p>
with description.txt as below:
<verb>
# Nodes
node1    00:02:73:49:9C:8D    192.168.10.1
node2    00:02:73:49:9C:8E    192.168.10.2
...	 ...		      ...

\#

# Disk & Parts
hda  80000
1    5000    swap
2    20000   debian
...  ...     ...
</verb>
<!-- =========================== -->
<sect1>kadelnode<label id="kadelnode">
<p>
<sect2>NAME
<p>
kadelnode - unregisters nodes
<sect2>SYNOPSIS
<p>
<tt>kadelnode -m|--machine hostname</tt>
<sect2>DESCRIPTION
<p>
kadelnode unregisters nodes and their disk and partition features from the deployment system.
<sect2>OPTIONS
<p>
<descrip>
<tag>-m, --machine hostname</tag> specifies a host. This option can be used more than once to specify several hosts
</descrip>
<sect2>EXAMPLE
<p>
<descrip>
<tag> kadelnode -m node1</tag> remove all information relative to node1 from the deployment system database
</descrip>
<!-- =========================== -->
<sect1>kadeploy<label id="kadeploy">
<p>
<sect2>NAME
<p>
kadeploy - deploys an environment on the given nodes
<sect2>SYNOPSIS
<p>
<tt>kadeploy -e|--environment environment_name -m|--machine hostname -p|--partition partition</tt>
<sect2>DESCRIPTION
<p>
kadeploy deploys the specified environment on the requested nodes. When successfully over, the nodes are available under the new and freshly installed environment.
<p>
It needs critic variables to be defined in the /etc/kadeploy/deploy.conf configuration file ; see configuration file section for further details.
<sect2>OPTIONS
<p>
<descrip>
<tag>-e, --environment environment_name</tag>the name of the environment to deploy
<tag>-m, --machine hostname</tag>specifies a host. This option can be used more than once to specify several hosts. By default, it tries to deploy on all the nodes.
<tag>-p, --partition partition</tag>the target partition. This option can't be used more than once and the partition is the same for all the nodes.
</descrip>
<sect2>CONFIGURATION FILES
<p>
<descrip>
<tag>/etc/kadeploy/deploy.conf</tag> Contains variables that should be defined prior to the command execution. These variables are grouped into several sections. A very short description is available for each variable and sometimes example values are given in commentary. The file is read once at the beginning of the execution and prior to any other action. If values are missing, it should be told as precisely as possible. The syntax is the following :
<verb>
# section_name #
# ------------ #

# short description
var_name1 = var_value1

# short description
var_name2 = var_value2	# example_value

...
</verb>
Also, an example of this file can be found at in the <tt>tools/cookbook/</tt> folder.

<tag>/etc/kadeploy/deploy_cmd.conf</tag> Contains entries describing reboot command needed for deployment. Please see kareboot man page for further details.
</descrip>
<sect2>FILES
<p>
kadeploy needs read and write access to several folder and files to acheive a deployment. These rights should be granted to the deploy user. Here are the files and folder list :
<itemize>
  <item><tt>$KADEPLOYDIR/boot/</tt> contains utilities (binaries etc.) for grub/pxe/reboot stories
  <item>the <tt>tftp</tt> folder where grub and pxe files will be written and that should be defined in the <tt>/etc/kadeploy/deploy.conf</tt> configuration file
  <item>and <tt>/tmp/</tt> for a few temporary files
</itemize>

<sect2>EXAMPLE
<p>
<descrip>
<tag>kadeploy -e debian -m node7 -p hda3</tag> deploys the debian on partition hda3 of node 7
<tag>kadeploy -e debian -m node1 -m node2 -p hdb6</tag> deploys the debian on partition hdb6 of nodes 1 and 2
</descrip>
<!-- =========================== -->
<sect1>kareboot<label id="kareboot">
<p>
<sect2>NAME
<p>
kareboot - reboots a node
<sect2>SYNOPSIS
<p>
<tt>kareboot [-n|--noreboot] [-s|--soft] [-h|--hard] [-d|--deploy] -m|--machine hostname [-e|--environment name] [-p|--partition partition]</tt>

<sect2>DESCRIPTION
<p>
kareboot can execute software or hardware reboots on given nodes. It can execute five types of query : no reboot, simple reboot, deploy reboot, environment reboot and partition reboot. For all of them, the user can specify one or several hosts.
<p>
Here is a brief description of the effects of each reboot type :
<itemize>
  <item>no reboot : setups pxe, grub and tftp stuff for the given nodes without rebooting
  <item>simple reboot : executes a 'normal' (classic) reboot of the given nodes
  <item>deploy reboot : reboots the given nodes on the deployment kernel
  <item>environment reboot : looks for the specified environment on the given nodes and reboots on it. The environment should thus already be installed.
  <item>partition reboot : reboots the given nodes on the requested partition if an appropriate environment is found
</itemize>
Consult the example section below for more explicit usage descriptions.

kareboot needs the appropriate command (softboot, hardboot, deployboot) to be defined in the configuration file in order to be able to execute the requested reboot on the given node ; see configuration file section for further details.

<sect2>OPTIONS
<p>
<descrip>
<tag>-n, --noreboot</tag>setups pxe, grub and tftp stuff
<tag>-s, --soft</tag>requests a software reboot (default reboot type)
<tag>-h, --hard</tag>requests a hardware reboot
<tag>-d, --deploy</tag>specify to reboot on the deployment kernel
<tag>-m, --machine hostname</tag>specifies a host. This option can be used more than once to specify several hosts.
<tag>-e, --environment</tag>gives an (already installed) environment to boot. When used, only one environment can be specified.
<tag>-p, --partition</tag>gives a partition to boot on. When used, only one partition can be specified.
</descrip>
<sect2>CONFIGURATION FILE
<p>
<descrip>
<tag>/etc/kadeploy/deploy_cmd.conf</tag>Can contain four entry types per node whose three are relative to the kareboot command. Those three entries describe the reboot commands for the soft, hard and deploy reboot type. Here is the description of the expected syntax : 
<p>
<tt>host_name reboot_type command</tt>
<p>
where reboot_type can be either softboot, hardboot or deployboot. 
<p>
Example for idpot1 of idpot cluster :
<itemize>
  <item><tt>idpot1 softboot ssh root@idpot1 -o ConnectTimeout=2 /sbin/reboot</tt>
  <item><tt>idpot1 hardboot ssh localuser@ldap-idpot ~/commande_modules/logiciel/dio_reset 0x81 0</tt>
  <item><tt>idpot1 deployboot ../boot/setup_pxe.pl 192.168.10.1:label_deploy_tg3</tt>
</itemize>
<p>
In this example, 
<itemize>
  <item>soft reboot is actually a simple "reboot" command done via a ssh connection as root
  <item>hard reboot is a hard signal sent via a ssh connection on a precise server as a special user
  <item>deploy boot is a call to the (deployment system) setup_pxe.pl perl script with appropriate arguments
</itemize>
</descrip>
<sect2>EXAMPLE
<p>
<descrip>
<tag>No reboot</tag>
  <descrip>
  <tag>kareboot -n -m node1 -e debian </tag>setups pxe, grub and tftp stuff for rebooting node1 on debian environment (if already installed)
  <tag>kareboot -n -m node7 -p hda5 </tag>setups pxe, grub and tftp stuff for rebooting node7 on partition hda5 (if an environment is found)
  </descrip>
<tag>Simple reboot</tag>
  <descrip>
  <tag>kareboot -s -m node1</tag>executes a software reboot of node1
  <tag>kareboot -h -m node7 -m node6</tag>executes a hardware reboot of nodes 7 and 6
  <tag>kareboot -m node7</tag>executes a software reboot of node7
  </descrip>
<tag>Deploy reboot</tag>
  <descrip>
  <tag>kareboot -d -m node2</tag>(soft) reboots on deployment kernel
  <tag>kareboot -d -h -m node2</tag>(hard) reboots on deployment kernel
  </descrip>
<tag>Environment reboot</tag>
  <descrip>
  <tag>kareboot -e debian -m node3</tag>looks for environment debian on node3 and (soft) reboots on it if it exists
  <tag>kareboot -h -e debian -m node3</tag>looks for environment debian on node3 and (hard) reboots on it if it exists
  </descrip>
<tag>Partition reboot</tag>
  <descrip>
  <tag>kareboot -s -p hda6 -m node4</tag>checks the environment installed on partition hda6 of node4 and (soft) reboots on it if appropriate
  </descrip>
</descrip>
<!-- =========================== -->
<sect1>kaconsole<label id="kaconsole">
<p>
<sect2>NAME
<p>
kaconsole - opens a console on a remote node
<sect2>SYNOPSIS
<p>
<tt>kaconsole -m|--machine hostname</tt>
<sect2>DESCRIPTION
<p>
kaconsole opens a console on a remote node. This tool is designed to help the user that would try to find out the reason of a deployment failure or to follow a boot sequence for instance. It needs the appropriate command to be defined in the configuration file in order to be able to open a remote console on the given node ; see configuration file section for further details.
<sect2>OPTIONS
<p>
<descrip>
<tag>-m, --machine hostname</tag>specifies a host. This option cannot be used more than once.
</descrip>
<sect2>CONFIGURATION FILE
<p>
<descrip>
<tag>/etc/kadeploy/deploy_cmd.conf</tag>Can contain four entry types per node whose only one is relative to the kaconsole command. This entry describes the command to open a remote console on the specified node of the cluster. Here is the description of the expected syntax :
<p>
<tt>host_name console command</tt>
<p>
Example for idpot1 of idpot cluster :
<p>
<tt>idpot1 console ssh -t localuser@ldap-idpot kermit -l /dev/ttyUSB0 -b 38400 -c</tt>
<p>
In this example, kaconsole -m idpot1 means to open a serial connection on idpot1 node via kermit software from ldap-idpot
</descrip>
<sect2>EXAMPLE
<p>
<descrip>
<tag>kaconsole -m node</tag>opens a console on node (if appropriately equipped)
</descrip>
<!-- =========================== -->
<sect1>karecordenv<label id="karecordenv">
<p>
<sect2>NAME
<p>
karecordenv - registers a environment
<sect2>SYNOPSIS
<p>
<verb>
karecordenv
   -n |  --name        registration name
   -v |  --version     version            # default is 1
   -d |  --description description
   -a |  --author      author email
   -fb| --filebase    environment image path
   -ft| --filesite    post-installation file path
   -s |  --size        size (Mo)
   -i |  --initrdpath  initrdpath        # default is none
   -k |  --kernelpath  kernel path
   -p |  --param       kernel param 
   -fd| --fdisktype   fdisk type         # default is 82
   -fs| --filesystem  file system        # default is ext2
</verb>
Name, kernel_path, environment_image_path and post-installation_file_path must be defined
<sect2>DESCRIPTION
<p>
karecordenv registers an environment in order to be able to use it with the deployment system.
<sect2>OPTIONS
<p>
<descrip>
<tag>-n,  --name registration_name</tag>the registration name for the environment
<tag>-v,  --version version</tag>the version number of the environment (if needed) ; default is 1
<tag>-d,  --description "description"</tag>a brief description of the environment
<tag>-a,  --author author_email</tag>the author email address
<tag>-fb, --filebase environment_image_path</tag>the complete path to the environment image
<tag>-ft, --filesite post-installation_file_path</tag>the complete path to the post-installation file
<tag>-s,  --size size (Mo)</tag>the size of the environment ; in order to perform partition size check before a deployment
<tag>-i,  --initrdpath</tag>the complete initrd path in the environment (including intrd name)
<tag>-k,  --kernelpath kernel_path</tag>the complete kernel path in the environment (including kernel name)
<tag>-p,  --param</tag>arguments passed to kernel at boot (<verb>root</verb> parameter is set automatically), if left empty, default <ref id="kernel_param" name="kernel parameters"> can be configured
<tag>-fd, --fdisktype fdisk_type</tag>the fdisk type ; default is 82
<tag>-fs, --filesystem file_system</tag>the file system type ; default is ext2
</descrip>
<sect2>EXAMPLE
<p>
<descrip>
<tag>karecordenv -n debian -v 2 -d "debian maison prte  l'installation" -a katools@imag.fr -fb file://home/nis/jleduc/ImagesDistrib/image_Debian_current.tgz -ft file://home/nis/jleduc/Boulot/postinstall/traitement.tgz -size 650 -k /boot/vmlinuz -i /initrd -p "console=ttyS0,38400n8 ETH_DRV=tg3" -fd 83 -fs ext2</tag>registers a debian image whose features are explicitly given in parameter
</descrip>
<!-- =========================== -->
<sect1>kaarchive<label id="kaarchive">
<p>
<sect2>NAME
<p>
kaarchive - creates an environment image
<sect2>SYNOPSIS
<p>
<tt>
kaarchive [-X | --exclude-from file] [-z | --gzip] [--output-directory output_directory] [-i | --image image_name] --root-directory root_directory
</tt>
<sect2>DESCRIPTION
<p>
kaarchive creates a environment image from a running system one.
<sect2>OPTIONS
<p>
<descrip>
<tag>-X, --exclude-from file</tag>excludes files named into file from the generated environment
<tag>-z, --gzip</tag>filters the archive through gzip
<tag>--output-directory output_directory</tag>sets the output directory for the generated environment ; default is current directory
<tag>-i, --image image_name</tag>sets the output image name ; default is output_image
<tag>--root-directory root_directory</tag>sets the root directory for the environment creation
</descrip>
<sect2>EXAMPLE
<p>
<descrip>
<tag>kaarchive -z -X /path/to/files_to_exclude_list.txt -i debian --root-directory /mnt/debian/</tag>creates the debian.tgz image file from /mnt/debian excluding the files listed into /path/to/files_to_exclude_list.txt
</descrip>

<!-- =========================== -->
<sect1>kacreateenv<label id="kacreateenv">
<p>
<sect2>NAME
<p>
kacreateenv - creates and registers a new environment
<sect2>SYNOPSIS
<p>
<verb>
kacreateenv 
   -e | --environment environment_name 
   -v | --version version 
   -X | --exclude-from file 
   -z | --gzip
   --output-directory output_directory
   --root-directory root_directory
   --author email_address
   --description "description"
   --file-base file
   --file-site file
   --size size
   --kernel-path kernel_path
   --fdisk-type fdisk_type_number
   --file-system file_system_type
</verb>
<sect2>DESCRIPTION
<p>
kacreateenv creates and registers a new environment from a running system one.
<sect2>OPTIONS
<p>
<descrip>
<tag>-e,  --environment environment_name</tag>the registration and output image name for the environment
<tag>-v,  --version version</tag>the version number of the environment (if needed) ; default is 1
<tag>-d,  --description "description"</tag>a brief description of the environment
<tag>-a,  --author author_email</tag>the author email address
<tag>-fb, --filebase environment_image_path</tag>the complete path to the environment image
<tag>-ft, --filesite post-installation_file_path</tag>the complete path to the post-installation file
<tag>-s,  --size size (Mo)</tag>the size of the environment ; in order to perform partition size check before a deployment
<tag>-k,  --kernelpath kernel_path</tag>the complete kernel path in the environment
<tag>-fd, --fdisktype fdisk_type_number</tag>the fdisk type ; default is 82
<tag>-fs, --filesystem file_system_type</tag>the file system type ; default is ext2
<tag>--root-directory root_directory</tag>sets the root directory for the environment creation
<tag>-X, --exclude-from file</tag>excludes files named into file from the generated environment
<tag>-z, --gzip</tag>filters the archive through gzip
<tag>--output-directory output_directory</tag>sets the output directory for the generated environment ; default is current directory
</descrip>
<sect2>EXAMPLE
<p>
<descrip>
<tag>kacreateenv -e toto_env --description "toto's special image" --author toto@fai.fr -fb file://home/images_folder/toto_env.tgz -ft file://home/images_folder/toto_spec_file.tgz --size 650 -k /boot/vmlinuz --root-directory /home/toto_img/ -X /path/to/file_to_exclude.txt -z</tag> creates and registers the environment according to the parameters
</descrip>

  </article>
