<!doctype linuxdoc system>
<article>
<!-- Initially generated from The_LDP_HOWTO_Generator V0.51 -->
<title>Kadeploy v2
<author>Julien Leduc (<url url="mailto:julien.leduc@imag.fr" name="julien.leduc@imag.fr">), 
Gaetan Peaquin, 
Johann Peyrard (<url url="mailto:jpeyrard@imag.fr" name="jpeyrard@imag.fr">)
<date>V0.02  2005-09-16
<!-- Primary category: 2.3. {Clustering} -->
<!-- Keywords: cluster,  deployment, kadeploy, perl, mysql, on demand, computing environment -->
<!-- Oneliner: Explains howto use kadeploy deploying system -->

<abstract>
<nidx>kadeploy</nidx>
The purpose of this HOWTO is to provide all the necessary steps to install and use kadeploy on demand deploying system in its second version.</abstract>
<toc>  <!-- generate a table of contents here -->
<!-- Space inserted for revision history (using RCS etc.)

     End of revision history -->

<sect>Introduction
<p>
<nidx>kadeploy!introduction</nidx>
Deploying multiple computing environment on clusters or grids cannot be easily handled with current tools that are often designed to deploy a single system on each node. An extension to this system is to allow the deployment of multiple computing environment on every node, without compromising the original system/boot sector.

<sect1>Copyright
<p>
Copyright &copy; 2004-07-20 by Julien Leduc, Gaetan Peaquin, Peyrard Johann.
You are free:
<itemize><item>to copy, distribute, display, and perform the work
<item>to make derivative works
<item>to make commercial use of the work
</itemize>Under the following conditions:
 Attribution. You must give the original author credit.
 Share Alike. If you alter, transform, or build upon this work, you may distribute the resulting work only under a license identical to this one.
<itemize><item>For any reuse or distribution, you must make clear to others the license terms of this work.
<item>Any of these conditions can be waived if you get permission from the author.</itemize>


<sect1>Disclaimer
<p>
Use the information in this document at your own risk. I disavow any potential liability for the contents of this document. Use of the concepts, examples, and/or other content of this document is entirely at your own risk.

All copyrights are owned by their owners, unless specifically noted otherwise. Use of a term in this document should not be regarded as affecting the validity of any trademark or service mark.

Naming of particular products or brands should not be seen as endorsements.

You are strongly recommended to take a backup of your system before major installation and backups at regular intervals.


<!-- =========================== -->
<sect1>News
<p>



<!-- =========================== -->
<sect1>Credits
<p>



<!-- =========================== -->
<sect1>Translations
<p>




<!-- =========================== -->
<sect>Overview
<p>
Kadeploy is a deployment system suite written in perl/shell/c.
It use a mysql databases for nodes information storing, and 
dhcp/tftp for pxe booting.
The package include kadeploy scripts, mysql schema, 
tools for pxe, deployment kernel (linux 2.4).


<sect1>Why use a database ?
<p>
kadeploy uses a database to maintain the persistance of the information about the cluster composition and current state.
<p>
The cluster composition is described via :
<itemize>
  <item> the nodes and their features (name, mac address and ip address)
  <item> the type of hard disk drive (size and device name) and their partitions (size and number)
  <item> the environments available (registered) for deployment with all the relative needed information (path to image, kernel path, etc.)
</itemize>
<p>
The cluster state is described via :
<itemize>
  <item> a snapshot of the cluster state that describes for each partition of each disk and on each node, the partition state, the environment installed on (if any), the number of the last deployment done on it and a brief error description about this deployment (if any)
  <item> the history of the ordered deployment with their start and end dates and their state
</itemize>
<!-- =========================== -->
<sect1>How does the database evolve during a deployment execution ?
<p>
During a deployment execution, its state and the one of the partition involved changes in the database.
<p>
A deployment can be in one of the following states :
<itemize>
  <item> waiting (for the nodes)
  <item> deploying
  <item> terminated
  <item> error
</itemize>
<p>
By convention, there cannot be more than one deployment waiting for nodes. 
<p>
A partition can be in one of the following states :
<itemize>
  <item> to deploy
  <item> deploying
  <item> rebooting
  <item> deployed
  <item> error
</itemize>
<p>
By convention, a node involved in a running deployment cannot take part in another deployment.
<p> 
The two conventions above guarantee that a node cannot be involved in two different deployments at the same time even on different disks and/or partitions.
<p>
When a deployment ends :
<itemize>
  <item> if it has failed on (at least) one node, it is changed to error state. When possible, a brief description of what (or when it) happened is associated to the matching partition ; the ones for which the deployment ended successfully are switched to the 'deployed' state. Thus, during a same deployment, two failures that would occur on two different partitions for different reasons would have different error messages. This enables to know if a failure is due to a global problem or to different local ones.
  <item> on the contrary, if the deployment ended successfully on all the nodes, it is changed to the 'terminated' state. The nodes are ready to be used on the new and freshly deployed environment.
</itemize>




<!-- =========================== -->
<sect1><label id="TFTP_structure"><tt>TFTP</tt> structure
<p>
Here is an example of a <tt>TFTP</tt> structure:
<verb>
/tftpboot/
   PXEClient/
      images_grub/
      pxelinux.cfg/
      pxelinux.0
      messages
      help.txt
   X86PC/
      linux/
         images_grub/
	 pxelinux.cfg/
	 linux.0
	 nbplinux.0
	 messages
	 help.txt
</verb>

This directory structure and allow to respond to different <tt>PXE</tt> request schemes. 
The slight differences are not our business here, you should refer to the different <tt>PXE</tt> standards for this part. 
Let's have a short look at this structure :
<itemize>
   <item><verb>/tftpboot/</verb> is <tt>TFTP</tt> root directory ; it is the directory served by the tftp server
   <item><verb>/tftpboot/PXEClient/</verb> is the tftp directory for non Intel NICs (set by <ref id="tftp_repository" name="tftp_repository"> in the configuration file)
   <item><verb>/tftpboot/X86PC/linux/</verb> is the tftp directory for Intel NICs (set by <ref id="tftp_repository_intel" name="tftp_repository_intel"> in the configuration file)
</itemize>
These two last directories contain both about the same structure :
<itemize>
   <item><verb>images_grub/</verb> that will contain <em>kadeploy</em> generated grub bootloaders, to allow the nodes to boot on the deployed system image (set by <ref id="tftp_relative_path" name="tftp_relative_path"> in the configuration file)
   <item><verb>pxelinux.cfg/</verb> that links every node to its kernel/initrd pair to load and boot from network, an ensure that we can remotely control the way the nodes are booting (set by <ref id="pxe_rep" name="pxe_rep"> and <ref id="pxe_rep_intel" name="pxe_rep_intel"> in the configuration file)
</itemize>

These four directories (two in each tftp directory) should be writtable by the <tt>deploy</tt> user, to allow <em>kadeploy</em> to control the way the nodes are booting. 














<!-- =========================== -->
<sect>Installation
<p>
The goal of this part is to ease the deployment system installation steps and give information about all the relevant required tools and their configuration.




<!-- =========================== -->
<sect1>Perl
<p>
A decent version of Perl need to be installed on the box. 
Actually the 5.8 works fine.
<p>
Package name : perl, perl-base, perl-mysql, perl-dbi, perl-suid, libterm-readkey-perl.
<!-- =========================== -->

<sect1>Mysql
<p>
Kadeploy has been tested with mysql 4.1.
<p>
Package name : mysql-server, mysql-client, libmysql, mysql-shared, 

<sect1>Sudo
<p>
In order to secure the system, you need sudo package.
It provide ways to grant access to simple users. 

<sect1>dhcpd
<p>
PXE boot is provided by ISC dhcpd software.
Use the package of your distributions.
A simple <tt>dhcpd.conf</tt> file is provided modify it for your needs <tt>tools/cookbook/conf/</tt>.

<sect1>nmap
<p>
Nmap is a tool for scanning remote system.
Kadeploy use it, because it is fast and scalable.

<sect1>Inetd/Xinetd
<p>
Daemon launcher use by many linux distribution.

<sect1>SYSLINUX
<p>
It is include in kadeploy package <tt>libboot/pxelinux/</tt>.
It contains a lot of booloader. 
<p>Pxelinux.0 (pxe bootloader), 
<p>Memdisk (floppy/hard drive image loader ). 
<p>In our case we use <tt>pxelinux.0</tt> from <url url="http://syslinux.zytor.com" name="syslinux">. 
This file is downloaded by the client, then launch, finally it download is conf from the pxelinux.cfg 
directory (see the syslinux doc for more).

<sect1>TFTP server
<p>
particulary <em>tftp-hpa</em> server also available under package.

<!-- =========================== -->
<sect1>Network configuration
<p>
This Howto will suppose you are building a cluster on a private Ip network.
The cluster nodes have Ip from 192.168.0.1 to 192.168.0.250.
The gateway, dns, dhcp, tftp, mysql services are located on the node 192.168.0.254.


<sect2>Database creation
<p>
The mysql schema are in <tt>share/mysql/</tt>.
You have to feed mysql with creation files, and patch.


<p><tt># mysql -u root -p < share/mysql/deploy_db_creation.sql</tt>
<p><tt>Enter password: ********</tt>

<p><tt># mysql -u root -p < share/mysql/rights_db_creation.sql</tt>
<p><tt>Enter password: ********</tt>

<p><tt># mysql -u root -p < share/mysql/patch_kadeploy_version.sql<tt>
<p><tt>Enter password: ********</tt>

Next step will be to fill in the database. 
That can be done easily using the kaaddnode command.
See the man pages for more about it.

<!-- =========================== -->
<sect2>How to configure access to databse ?
<p>
In order for the deployment system to access to the database, correct information e.g. hostname, database name, login and password should be filled in the configuration file for each database. 



<!-- =========================== -->
<sect1><tt>PXE/TFTP</tt>
<p>
This part explains how to configure <tt>PXE/TFTP</tt> to control the booting process on the cluster nodes.
<!-- =========================== -->
<sect2>Hardware specifications
<p>
The nodes' hardware must be <tt>PXE</tt> compliant, this means that the <tt>BIOS/EFI</tt> should allow to boot from the ethernet card. To ensure that, you must verify that the network interface is <tt>PXE</tt> compliant.
<!-- =========================== -->



<sect2><label id="dhcpd.conf"> <tt>dhcpd.conf</tt> example
<p>
An example of server configuration.
<verb>
allow booting;
allow bootp;
deny unknown-clients;

option domain-name "mycluster.net";
option domain-name-servers 192.168.0.254;

option subnet-mask 255.255.255.0;
default-lease-time 600;
max-lease-time 7200;

subnet 192.168.0.0 netmask 255.255.255.0 {
  range 192.168.0.0 192.168.0.250;
  option broadcast-address 192.168.0.255;
  option routers 192.168.0.254;
}


host cls1 {
  hardware ethernet 00:01:02:04:73:da;
  fixed-address 192.168.0.1;
  filename "pxelinux.0";
  server-name "cls1";
}

host cls2 {
  hardware ethernet 00:01:02:02:a7:f5;
  fixed-address 192.168.0.2;
  filename "pxelinux.0";
  server-name "cls2";
}
</verb>

<!-- =========================== -->
<sect2><label id="inetd.conf"> <tt>inetd.conf</tt> example
<p>
An example of inetd.conf
<verb>
tftp            dgram   udp     wait    nobody /usr/sbin/tcpd /usr/sbin/in.tftpd --tftpd-timeout 300 --retry-timeout 5 --maxthread 100 --verbose=5  /tftpboot
</verb>




<!-- =========================== -->
<sect1>configure && make && makeinstall
<p>
<em>kadeploy</em> have to be install somewhere on your system.
<itemize>
 <item>This can be done by a simple <tt>./configure --prefix=/usr/local && make && make install
 <item>The bin sbin content need to be accessible from the path (symlink or correct your prefix).
 <item>A line like this one must be in your <tt>/etc/kadeploy/deploy.conf</tt>
  <verb>kadeploy2_directory = /my_path_to_kadeploy_directory/</verb>
 <item>kadeploy need some lib perl, "pm" files.
This files are located in <tt>$prefix/share/perl/5.8/libkadeploy2/</tt>.
You can symlink it in <tt>/usr/lib/perl/5.8/</tt> (or a directory included in your local Perl @INC).
</itemize>


<sect1>Configuration files.
<p>
It is mandatory to create a <tt>/etc/kadeploy</tt> directory, and put
the two configuration file in it.
<itemize>
 <item><tt>/etc/kadeploy/deploy.conf</tt> contain tools and directory path, 
global variable, database information, and kernel boot parameters.
 <item><tt>/etc/kadeploy/deploy_cmd.conf</tt> contain the command to be exectuted for each node.
</itemize>
Example of each file can be found in <tt>tools/cookbook/conf/</tt>.


<!-- =========================== -->
<sect1>System Requirements
<p>
<!-- =========================== -->
<sect2>Required privileges to execute <em>kadeploy</em>
<p>
<em>kadeploy</em> requires to perform some changes on the server, to control the way the nodes are booting. 
Some other tools like <em>kaconsole</em> or <em>kareboot</em> needs to connect to other computers. 
The purpose of this part is to define all the required privileges to ensure a smooth execution of <em>kadeploy</em>, and secure it by avoiding unnecessary privileges.

<!-- =========================== -->
<sect>Using kadeploy 2
<p>
The deployment system is composed of several tools or commands whose aim is the execution of all the needed actions to carry out deployments and other administrative tasks.
Every tool has his own man page.

<!-- =========================== -->
<sect1>The first time...
<p>
The two first steps are to register the cluster "software and hardware composition" in its current state. 
<p>
If any environment is already installed on some partitions of some nodes, it is necessary to register it. 
This operation is done by giving the appropriate information (image name, image location, kernel path etc.) to the <ref id="karecordenv" name="karecordenv"> tool. 
<p>
Then, <ref id="kaaddnode" name="kaaddnode"> is used to register the cluster "hardware" composition. 
It needs the description of hosts, disks and partitions in a text file. 
The tool reads it and registers the information (name and addresses of nodes, disk type and size, partition number and size etc.) in the database.
<p>
Once these operations completed, the system is ready for deployments.


<p>
Example of <tt>cluster_description.txt</tt> file.
 <verb>
# Nodes description
cls1        00:01:02:04:73:da     192.168.0.1

\#

# Partition description

hda 8000
1   500  swap
2   3000 empty
3   500  tmp
5   2000 empty
6   2000 empty
</verb>

<p>
You need to execute the command:
<verb>
#kaaddnode /etc/kadeploy/cluster_description.txt
Checking variable definition...
Registration completed.
</verb>


<!-- =========================== -->
<sect1>Deployment
<p>
The system has been prepared for deployment. Now what ? To deploy, something to be deployed is needed !
<p>
The environment to be deployed can either already exist and be registered in the database or already exist but not be registered or neither exist nor be registered. The first case is the simplest since nothing has to be done prior to the deployment itself. In the other cases, <ref id="kacreateenv" name="kacreateenv"> (<ref id="kaarchive" name="kaarchive">/<ref id="karecordenv" name="karecordenv">) are used to create and register (create/register) an environment in the database (cf. the environment section for details about these steps).
<p>
The deployment itself i.e. of a given environment on target partitions of a set of cluster nodes is done using the <ref id="kadeploy" name="kadeploy"> tool.
<p>
A complete deployment is composed of the following steps :
<itemize>
  <item>reboot on the deployment kernel via pxe protocol
  <item>pre-installation actions (bench, partitionning and/or file system building if needed etc.)
  <item>environment copy
  <item>post-installation script sending and execution
  <item>reboot on the freshly installed environment
</itemize>
<p>
If the deployment fails on some nodes, these are rebooted on a default environment.
<p>
Please refer to the <ref id="About_the_customization_scripts" name="customization scripts section"> for further details about preinstall and postinstall scripts.
<!-- =========================== -->

<sect1>Preinstall & Postinstall
<p>
The preinstall and postinstall are tarball archive.
They contains many files AND a <tt>main</tt> scripts.
They are pushed on the node, extracted, and the <tt>main</tt> script is launched.
(see <tt>/etc/kadeploy/deploy.conf</tt> for setting tarball and script).
<itemize>
 <item>Preinstall contains actions that are needed in order to deploy the image on the node.
fdisk, mkfs, mkswap, mount, bench...
 <item>Postinstall contains actions that can't be put in the preinstall scripts.
Manage the fstab, the ssh services, and what you think important for a node...
</itemize>
<p>
Actually you have to make yourself your fdisk.txt files...
This file is liked a fdisk, script.
Not hard but boring, it will be fix sooner.


<!-- =========================== -->
<sect1>Cluster BootStrap
<p>
You need a tarball of a linuxbox.
something like <tt>myLinux.tgz</tt> that can be created simply like this:
<verb>
#cd /
#tar czvlf /pathToAnotherFileSystem/myLinux.tgz /
</verb> 
<p>
You now need to record this environement in kadeploy database.
<verb>
#karecordenv 
            -n "myLinux" 
            -v 2 
            -d "Newbie testing" 
            -a noobs@mycluster.net 
            -fb file://home/noobs/myLinux.tgz 
            -ft file://home/noobs/traitement.tgz  
            -s 900 
            -i /boot/initrd.img-2.6.8-1-686-smp 
            -k /boot/vmlinuz-2.6.8-1-686-smp
</verb>

<p>
Now you have got, an environnement, you have to watch for a preinstall, postinstall archive.
An example is given in <tt>kadeploy2/tools/cookbook/</tt>.

<p>
The cluster need a reference environnement to boot up properly.
It's a chicken eggs problem.
A solution, is 
<itemize>
<item>Set the nodes in deployement state with the <tt>pxe_setup.pl</tt> tools (grep label in deploy.conf).
 <verb>#setup_pxe.pl 129.88.69.111:label_deploy_ecluster_hda2 </verb>
<item>Test the communication with <tt>rsh</tt>.
 <verb>#rsh nodeName /bin/ash </verb>
<item>fdisk the node.
 <verb>#fdisk /dev/hda </verb>
<item>mkfs the node.
 <verb>#mkfs /dev/hda1 </verb>
<item>mount the reference partition.
 <verb>#mkdir /mnt/hda1 && mount /dev/hda1 /mnt/hda1 </verb>
<item>push a reference image on it.
 <verb>rsh nodeName "cd /mnt/hda1 ; tar xzvf -" < /pathTo/myLinux.tgz </verb>
<item>umount it.
 <verb>#rsh nodeName /sbin/umount /mnt/hda1 </verb>
<item>Set the node in reference state with <tt>pxe_setup.pl</tt>.
 <verb>#setup_pxe.pl 129.88.69.111:label_deploy_ecluster_ref </verb>
<item>reboot the node.
 <verb>#rsh nodeName /sbin/reboot </verb>
</itemize>

<p>
Now you can ssh the node, and kadeploy a new environnement.
If you want to simplify your administrator life.
Use a sudoers and sudowrapper.sh for kadeploy, a simple one is in <tt>tools/cookbook/conf/</tt>.
set ssh in public key authentification for the master and the compute nodes.




<!-- =========================== -->
<sect1>Other tools for remote management
<p>
Two remote management tools are available to diagnostic and if possible take control of cluster nodes that would be for instance in an unstable or undefined state further to a deployment failure. 
<p>
The <ref id="kaconsole" name="kaconsole"> tool enables to open a console on a remote node. It needs special hardware equipment and special command configuration as described in the second part of the configuartion files section.
<p>
The <ref id="kareboot" name="kareboot"> tool enables to do various reboot types on cluster nodes. The possibilities are numerous : the reboot on a given (already installed) environment or on a given partition or even on the deployment kernel for instance. It also enables to hard reboot nodes appropriately equiped.
<!-- =========================== -->
<sect1>What if the cluster hardware composition changes ?
<p>
<ref id="kaaddnode" name="kaaddnode"> and <ref id="kadelnode" name="kadelnode"> enable to add and remove nodes from the deployment system if the cluster composition changes.
<!-- =========================== -->
<sect1>Command summary 
<p>
<sect2>Functionalities summary
<p><tt>kaaddnode</tt> - registers nodes in deployment system
<p><tt>kadelnode</tt> - unregisters nodes in deployment system
<p><tt>karecordenv</tt> - registers an environment image in deployment system
<p><tt>kaarchive</tt> - creates an environment image
<p><tt>kacreateenv</tt> - creates and registers an environment image in deployment system
<p><tt>kadeploy</tt> - deploys an environment image
<p><tt>kaconsole</tt> - opens a remote console 
<p><tt>kareboot</tt> - reboots according to requested reboot type
<p>
<sect2>Use summary
<p>The first time...
<itemize>
  <item<tt>karecordenv</tt> - to register environments already installed (if any)
  <item><tt>kaaddnode</tt> - to register the cluster hardware composition
</itemize>
<p>Deployment
<itemize>
  <item><tt>kacreateenv/karecordenv/kaarchive</tt> - to create and/or register an environment and make it available for deployment
  <item><tt>kadeploy</tt> - to deploy a registered environment
</itemize>
<p>Other tools
<itemize>
  <item><tt>kaconsole</tt> - to open a console on a remote node
  <item><tt>kareboot</tt> - to reboot a cluster node
</itemize>
<sect2>Examples
<p>
<verb>
# karecordenv -n debian -d "custom debian" -fb file://home/toto/images/custom_debian.tgz -ft file://home/toto/images/debian_postinstall.tgz -size 750 -k /boot/vmlinuz 
# kaaddnode cluster_description.txt
# kacreateenv -e new_debian -fb file://home/toto/images/new_debian.tgz -ft file://home/toto/images/debian_postinstall.tgz --size 650 -k /boot/vmlinuz --root-directory /
# kadeploy -e new_debian -m node1 -m node2 -p hda7
# kaconsole -m node2
# kareboot -s -e custom_debian -m node2
</verb>

<!-- =========================== -->
<sect>About the customization scripts<label id="About_the_customization_scripts">
<p>
Kadeploy allows the customization of each node by 2 means:
<itemize>
  <item>preinstallation script, executed before sending the system image
  <item>postinstallation, executed after having sent the system image
</itemize>
Originally, these two scripts are written in <em>ash</em>, which is a lightweight bash, but the way these scripts are designed could allow to add any script language.

<!-- =========================== -->
<sect1>Preinstallation script
<p>
This script is common to all environments, its goal is to prepare the system to the hardware specification and the target hard disk drive for the deployment. It can load a specific IDE controller driver, improve deployment performance or make every kind of checks needed. This script is defined in the configuration file as <ref id="pre_install_script" name="pre_install_script"> and the associated archive as <ref id="pre_install_archive" name="pre_install_archive">.
<!-- =========================== -->
<sect2>Preinstallation archive structure
<p>
The preinstallation archive is a gzipped tar archive, containing the <ref id="pre_install_script" name="pre_install_script"> in its root directory.
Here is an example of a preinstallation archive structure:
<verb>
/
   init.ash
   lib/
   bin/
      awk
      df
      du
      xargs
</verb>

The directory structure allows to custom the tasks to your needs. In this example, the pre_install_script is <em>init.ash</em>. Let's have a short look at this structure :
<itemize>
   <item><verb>init.ash</verb> is my pre_install_script, so it needs to be there
   <item><verb>bin/</verb> is a directory where you can add custom binaries, here, I needed <em>awk</em>, <em>df</em>, <em>du</em> and <em>xargs</em>.
   <item><verb>lib/</verb> is a directory where you can add custom libraries for your binaries. I suggest you to compile static binaries, to prevent conflicts/version problems due to the presence of the system's shared libraries.
</itemize>

<!-- =========================== -->
<sect1>Postinstallation script
<p>
This script is associated to the environment to deploy. Its goal is to adapt the crude system image to a bootable system. It is composed of a gunzipped tar archive that contains all the sites files and a script <em>traitement.ash</em> in the archive's root directory. This archive is sent to the nodes, decompressed in a ramdisk and then the <tt>post_install_script</tt> is executed on every node. The script name is defined in the configuration file as <ref id="post_install_script" name="post_install_script">.
<!-- =========================== -->
<sect2>Postinstallation archive structure
<p>
The postinstallation archive is a gzipped tar archive, containing the <ref id="post_install_script" name="post_install_script"> in its root directory.
Here is an example of a postinstallation archive structure:
<verb>
/
   traitement.ash
   etc/
      fstab
      hosts
      hosts.allow
      hosts.deny
      ntpdate
   authorized_keys
</verb>

The directory structure allows to custom the configuration script to your needs. Let's have a short look at this structure :
<itemize>
   <item><verb>traitement.ash</verb> is my post_install_script.
   <item><verb>etc/</verb> is a directory where I decided to put all the files I wanted to replace on my system, this is an arbitrary choice, but allows to keep a clean structure.
   <item><verb>authorized_keys</verb> is the root's authorized_keys file I decided to put it in the root directory to be sure to have a look at it everytime I update my postinstallation archive.
</itemize>

<!-- =========================== -->
<sect2>System modifications
<p>
<enum>
   <item><verb>/etc/fstab</verb> a site base file should be copied in the postinstall archive so that nfs mounts can be preserved and other site modifications could be preserved
   <item><verb>/tmp</verb> should have its rights modified
</enum>
<!-- =========================== -->
<sect2>Administrative aspects
<p>
Many other basic files can be handled by putting them in the archive and replace the existing ones. They are not all listed here but only the most important ones :
<enum>
   <item><verb>/root/.ssh/authorized_keys</verb> should contain the administrator's public key and also the public key of user <tt>deploy</tt>, to allow him to get a root shell on every node to reboot those. In order to do that this authorized_keys file has to be built and put in the archive's root directory
   <item><verb>/etc/hosts /etc/hosts.allow /etc/hosts.deny</verb> should be set to fit the cluster's configuration, and ensure network connection within the cluster's network
</enum>

Numerous modifications can be done here, from authentification server to tailored modification depending on the node's IP. A good idea should be to modify <tt>rc</tt> scripts to prevent the first boot hard disk drive verification, because it is just a waste of time here, and avoid all the manual intervention that could occur on system boot : for example, by default, many distributions ask the root password before checking a filesystem on boot time.


<!-- =========================== -->
<sect>Configuration Files
<p>
<em>kadeploy</em> tools suite is configured through two configuration files : <tt>deploy.conf</tt> and <tt>deploy_cmd.conf</tt> in the <tt>/etc/kadeploy/</tt> folder.
<!-- =========================== -->
<!-- =========================== -->

  </article>
