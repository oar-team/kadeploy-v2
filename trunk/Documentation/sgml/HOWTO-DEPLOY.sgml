<!doctype linuxdoc system>
<article>
<!-- Initially generated from The_LDP_HOWTO_Generator V0.51 -->
<title>Kadeploy v2.1.6
<author>Julien Leduc (<url url="mailto:julien.leduc@imag.fr" name="julien.leduc@imag.fr">), 
Pierre Lemoine,
Gaetan Peaquin, 
Johann Peyrard (<url url="mailto:jpeyrard@imag.fr" name="jpeyrard@imag.fr">)
<date>V0.02  2005-09-16
<!-- Primary category: 2.3. {Clustering} -->
<!-- Keywords: cluster,  deployment, kadeploy, perl, mysql, on demand, computing environment -->
<!-- Oneliner: Explains howto use kadeploy deploying system -->

<abstract>
<nidx>kadeploy</nidx>
The purpose of this HOWTO is to provide all the necessary steps to install and use kadeploy on demand deploying system in its second version.</abstract>
<toc>  <!-- generate a table of contents here -->
<!-- Space inserted for revision history (using RCS etc.)

     End of revision history -->

<sect>Introduction
<p>
<nidx>kadeploy!introduction</nidx>
Deploying multiple computing environment on clusters or grids cannot be easily handled with current tools that are often designed to deploy a single system on each node. An extension to this system is to allow the deployment of multiple computing environment on every node, without compromising the original system/boot sector.

<sect1>Copyright
<p>
Copyright &copy; 2004-07-20 by Julien Leduc, Gaetan Peaquin, Peyrard Johann.
You are free:
<itemize><item>to copy, distribute, display, and perform the work
<item>to make derivative works
<item>to make commercial use of the work
</itemize>Under the following conditions:
 Attribution. You must give the original author credit.
 Share Alike. If you alter, transform, or build upon this work, you may distribute the resulting work only under a license identical to this one.
<itemize><item>For any reuse or distribution, you must make clear to others the license terms of this work.
<item>Any of these conditions can be waived if you get permission from the author.</itemize>


<sect1>Disclaimer
<p>
Use the information in this document at your own risk. I disavow any potential liability for the contents of this document. Use of the concepts, examples, and/or other content of this document is entirely at your own risk.

All copyrights are owned by their owners, unless specifically noted otherwise. Use of a term in this document should not be regarded as affecting the validity of any trademark or service mark.

Naming of particular products or brands should not be seen as endorsements.

You are strongly recommended to take a backup of your system before major installation and backups at regular intervals.


<!-- =========================== -->
<sect1>News
<p>



<!-- =========================== -->
<sect1>Credits
<p>



<!-- =========================== -->
<sect1>Translations
<p>



<!-- =========================== -->
<sect>Overview
<p>
Kadeploy is a deployment system suite written in perl/shell/c.
It use a mysql databases for nodes information storing, and 
dhcp/tftp for pxe booting.
The package include kadeploy scripts, mysql schema, 
tools for pxe, deployment kernel (linux 2.4).


<sect1>Why use a database ?
<p>
kadeploy uses a database to maintain the persistance of the information about the cluster composition and current state.
<p>
The cluster composition is described via :
<itemize>
  <item> the nodes and their features (name, mac address and ip address)
  <item> the type of hard disk drive (size and device name) and their partitions (size and number)
  <item> the environments available (registered) for deployment with all the relative needed information (path to image, kernel path, etc.)
</itemize>
<p>
The cluster state is described via :
<itemize>
  <item> a snapshot of the cluster state that describes for each partition of each disk and on each node, the partition state, the environment installed on (if any), the number of the last deployment done on it and a brief error description about this deployment (if any)
  <item> the history of the ordered deployment with their start and end dates and their state
</itemize>
<!-- =========================== -->
<sect1>How does the database evolve during a deployment execution ?
<p>
During a deployment execution, its state and the one of the partition involved changes in the database.
<p>
A deployment can be in one of the following states :
<itemize>
  <item> waiting (for the nodes)
  <item> deploying
  <item> terminated
  <item> error
</itemize>
<p>
By convention, there cannot be more than one deployment waiting for nodes. 
<p>
A partition can be in one of the following states :
<itemize>
  <item> to deploy
  <item> deploying
  <item> rebooting
  <item> deployed
  <item> error
</itemize>
<p>
By convention, a node involved in a running deployment cannot take part in another deployment.
<p> 
The two conventions above guarantee that a node cannot be involved in two different deployments at the same time even on different disks and/or partitions.
<p>
When a deployment ends :
<itemize>
  <item> if it has failed on (at least) one node, it is changed to error state. 
When possible, a brief description of what (or when it) happened is associated to the matching partition ; 
the ones for which the deployment ended successfully are switched to the 'deployed' state. 
Thus, during a same deployment, two failures that would occur on two different partitions for different reasons would have different error messages. 
This enables to know if a failure is due to a global problem or to different local ones.
  <item> on the contrary, if the deployment ended successfully on all the nodes, it is changed to the 'terminated' state. 
The nodes are ready to be used on the new and freshly deployed environment.
</itemize>




<!-- =========================== -->
<sect1><label id="TFTP_structure"><tt>TFTP</tt> structure
<p>
Here is an example of a <tt>TFTP</tt> structure:
<verb>
/tftpboot/
   PXEClient/
      images_grub/
      pxelinux.cfg/
      pxelinux.0
      messages
      help.txt
   X86PC/
      linux/
         images_grub/
	 pxelinux.cfg/
	 linux.0
	 nbplinux.0
	 messages
	 help.txt
</verb>

This directory structure and allow to respond to different <tt>PXE</tt> request schemes. 
The slight differences are not our business here, you should refer to the different <tt>PXE</tt> standards for this part. 
Let's have a short look at this structure :
<itemize>
   <item><verb>/tftpboot/</verb> is <tt>TFTP</tt> root directory ; it is the directory served by the tftp server
   <item><verb>/tftpboot/PXEClient/</verb> is the tftp directory for non Intel NICs (set by <ref id="tftp_repository" name="tftp_repository"> in the configuration file)
   <item><verb>/tftpboot/X86PC/linux/</verb> is the tftp directory for Intel NICs (set by <ref id="tftp_repository_intel" name="tftp_repository_intel"> in the configuration file)
</itemize>
These two last directories contain both about the same structure :
<itemize>
   <item><verb>images_grub/</verb> that will contain <em>kadeploy</em> generated grub bootloaders, to allow the nodes to boot on the deployed system image (set by <ref id="tftp_relative_path" name="tftp_relative_path"> in the configuration file)
   <item><verb>pxelinux.cfg/</verb> that links every node to its kernel/initrd pair to load and boot from network, an ensure that we can remotely control the way the nodes are booting (set by <ref id="pxe_rep" name="pxe_rep"> and <ref id="pxe_rep_intel" name="pxe_rep_intel"> in the configuration file)
</itemize>

These four directories (two in each tftp directory) should be writtable by the <tt>deploy</tt> user, to allow <em>kadeploy</em> to control the way the nodes are booting. 














<!-- =========================== -->
<sect>Installation
<p>
The goal of this part is to ease the deployment system installation steps and give information about all the relevant required tools and their configuration.

First, get the kadeploy archive and put it on the your cluster frontend.

<sect1>Prerequisites
<p>
Install mysql (configure it)

Install dhcpd (configure it)

Install tftp  (configure it)

Install sudo

Install dns (optional)

<sect2>For Debian 
<p>

<sect2>For RedHat and Fedora
<p>

<sect1> Client part
<p>
For all commands you have to be root

Optionnaly before starting:
	The first thing that the Makefile does is to create a user 'deploy' in a group 'deploy' with '/home/deploy' for homedir. 
	If you want to specify a custom homedir, you have to do that by yourself before make install.
	The main thing is that you have a 'deploy' user in a 'deploy' group, home directory is not important for kadeploy (but can be for you).

In the current directory do :
<tt>make install</tt>

then you have to install tftp part. You can use variable by defaut:

<tt>make tftp_install</tt>

or specifies tftp install variables (pxe_rep and image_grub are relative to tftp_repository):

<tt>make ARCH=x86_64 tftp_repository=/var/lib/tftpboot/PXEClient/ pxe_rep=pxelinux.cfg/ tftp_relative_path=images_grub tftp_install</tt>

Now you have finished to install kadeploy client and you can edit kadeploy configuration file in /etc/kadeploy/deploy.conf

<sect1> DB part
<p>
Before doing DB install, client part has to be finished.

Edit <tt>/etc/kadeploy/deploy.conf</tt> to specify:  deploy_db_host 

If your database is on the same host as Kadeploy: deploy_db_host=localhost

If not, specify the remote host address: deploy_db_host=REMOTE_HOST_ADDRESS. For example, 192.168.1.10

In the current directory do:

<tt>make db_install</tt>

For Remote Host DataBase:
 
	You have to be able to make this command line without error:  

		<tt>mysql -u root -h REMOTE_HOST_ADDRESS -p --database=mysql </tt>

	If you cannot do that, check your /etc/mysql/my.cnf to see there is no line like this: bind-address= . If yes, remove this line.

	So connect to host where database is installed and connect to your database:

	   	<tt>mysql -u root -p</tt>

	And type this command:

		GRANT ALL PRIVILEGES ON *.* TO 'root'@'KADEPLOY_HOST_ADDRESS' IDENTIFIED BY '' WITH GRANT OPTION;
		FLUSH  PRIVILEGES;

	And exit from mysql. Reconnect to host where Kadeploy is installed and in the KADEPLOY HOME DIRECTORY (/usr/local/kadeploy by default) do:

	    	<tt>./kadeploy_db_init.pl</tt>

For Remote Host DataBase:

	connect to host where database is installed and connect to your database: mysql -u root -p

	And type this command:  DELETE FROM user WHERE user='root' && host='KADEPLOY_HOST_ADDRESS';

	And exit from mysql.

and follow instructions

Note: By default you can connect to deploy database from anywhere. It is not necessary, but you can reduce access to deploy database to your head node. It is more secure.

It's done !!

<sect> Configuration


<sect1><label id="dhcpd.conf"> <tt>dhcpd.conf</tt> example
<p>
An example of server configuration.
<verb>
allow booting;
allow bootp;
deny unknown-clients;

option domain-name "mycluster.net";
option domain-name-servers 192.168.0.254;

option subnet-mask 255.255.255.0;
default-lease-time 600;
max-lease-time 7200;

subnet 192.168.0.0 netmask 255.255.255.0 {
  range 192.168.0.0 192.168.0.250;
  option broadcast-address 192.168.0.255;
  option routers 192.168.0.254;
}


host cls1 {
  hardware ethernet 00:01:02:04:73:da;
  fixed-address 192.168.0.1;
  filename "pxelinux.0";
}

host cls2 {
  hardware ethernet 00:01:02:02:a7:f5;
  fixed-address 192.168.0.2;
  filename "pxelinux.0";
}
</verb>

<!-- =========================== -->
<sect2><label id="inetd.conf"> <tt>inetd.conf</tt> example
<p>
An example of inetd.conf
<verb>
tftp            dgram   udp     wait    nobody /usr/sbin/tcpd /usr/sbin/in.tftpd --tftpd-timeout 300 --retry-timeout 5 --maxthread 100 --verbose=5  /tftpboot
</verb>








<sect1>Configuration files.
<p>
It is mandatory to create a <tt>/etc/kadeploy</tt> directory, and put
the four configuration file in it.
<itemize>
 <item><tt>/etc/kadeploy/deploy.conf</tt> contain tools and directory path, 
global variable, database information, and kernel boot parameters.
 <item><tt>/etc/kadeploy/deploy_cmd.conf</tt> contain the command to be exectuted for each node.
 <item><tt>/etc/kadeploy/clusterpartition.conf</tt> contains default partitionment schema.
 <item><tt>/etc/kadeploy/clusternodes.conf</tt> contains the nodes informations.
</itemize>
Example of each file can be found in <tt>tools/cookbook/conf/</tt>.



<!-- =========================== -->
<sect1>The first time...
<p>
You have to feed mysql with kadeploy schema.
the <tt>kadatabase</tt> command was created for this.
<verb>#kadatabase -addmysqlrights -create_db_deploy -create_table_deploy -patch21 -patch211</verb>

<p>
You can test it with <tt>kaenvironments</tt> command.

<p>
Then, <ref id="kaanodes" name="kanodes"> is used to register the cluster "hardware" composition. 
It needs the description of hosts, disks and partitions in a text file. 
The tool reads it and registers the information (name and addresses of nodes, disk type and size, partition number and size etc.) in the database.

<p>
Example of <tt>/etc/kadeploy/clusternodes.conf</tt> file.
 <verb>
cls1 00:01:02:04:73:DA 192.168.0.1
cls2 00:01:02:02:A7:F5 192.168.0.2
</verb>

<p>
Example of <tt>/etc/kadeploy/clusterpartition.conf</tt> file.
 <verb>
hda size=80000
part=1 size=2500  fdisktype=82 label=empty type=primary
part=2 size=2000  fdisktype=83 label=empty type=primary
part=3 size=20000  fdisktype=83 label=empty type=primary
part=4 size=55000  label=empty type=extended

part=5 size=5000   fdisktype=83 label=empty type=logical
part=6 size=5000   fdisktype=83 label=empty type=logical
part=7 size=5000   fdisktype=83 label=empty type=logical
part=8 size=5000   fdisktype=83 label=empty type=logical
part=9 size=5000   fdisktype=83 label=empty type=logical
part=10 size=5000   fdisktype=83 label=empty type=logical
part=11 size=5000   fdisktype=83 label=empty type=logical
part=12 size=5000   fdisktype=83 label=empty type=logical
part=13 size=5000   fdisktype=83 label=empty type=logical
</verb>



<p>
You need to execute the command:
<verb>
#kanodes -add
Checking variable definition...

Deleting partition table
Checking /etc/kadeploy/clusternodes.conf
Registring cls1
Registring cls2
Nodes Registration completed.

Checking /etc/kadeploy/clusterpartition.conf
Registring harddisk hda
Registring part1 empty
Registring part2 empty
Registring part3 empty
Registring part5 empty
Registring part6 empty
Registring part7 empty
Registring part8 empty
Registring part9 empty
Registring part10 empty
Registring part11 empty
Registring part12 empty
Registring part13 empty
Register partition done.
</verb>

<p>
If you want to check if the db is correct, you can do:
#kanodes -listnode
#kanodes -listpartition

<p>
Once these operations completed, the system is almost ready for deployments.
You have to put some right to your user with the command kaaduser.
<verb>#kaadduser -l root -m cls2 -p hda2</verb>

<p>
Warning, when you change on of this two file you have to recall <tt>kanodes -add</tt>.


<sect1>Configure nodes(ajouter capture d'Ã©cran)
<p>
To make a deployment on node, you have to check its bios.
<itemize>
<item>Firstly, node must be able to use PXE boot. Find PXE item in bios and enable it.
<item>Secondly, search item usually named boot sequence. When you find it, put PXE (network,ethernet0) the first device to boot
</itemize>

<sect1>Preinstall 
<p>
The preinstall is a tarball archive.
It contains many files AND a <tt>main</tt> scripts.
They are pushed on the node, extracted, and the <tt>main</tt> script is launched.
The main script is found with : <tt> pre_install_script </tt> in configuration file.
(see <tt>/etc/kadeploy/deploy.conf</tt> for setting tarball and script).
Preinstall contains needed actions in order to deploy the image on the node (fdisk, mkfs, mkswap, bench)...
Some configuration files are pushed on the node (fdisk_user.txt and preinstall.conf). 


<!-- =========================== -->
<sect>Using kadeploy 2.1.6

<!-- =========================== -->
<sect1>Deployment (ajouter animation flash)
<p>
The system has been prepared for deployment. Now what ? To deploy, something to be deployed is needed !
<p>
The environment to be deployed can either already exist and be registered in the database or already exist but not be registered or neither exist nor be registered. 
The first case is the simplest since nothing has to be done prior to the deployment itself. 
In the other cases, <ref id="karecordenv" name="karecordenv">) are used to create and register (create/register) an environment in the database 
<p>
The deployment itself i.e. of a given environment on target partitions of a set of cluster nodes is done using the <ref id="kadeploy" name="kadeploy"> tool.
<p>
A complete deployment is composed of the following steps :
<itemize>
  <item>reboot on the deployment kernel via pxe protocol
  <item>pre-installation actions (partitionning and/or file system building if needed etc.)
  <item>environment copy
  <item>post-installation script sending and execution
  <item>reboot on the freshly installed environment
</itemize>
<p>
If the deployment fails on some nodes, these are rebooted on a default environment.
<p>
<!-- =========================== -->


<sect1>Postinstall
<p>
The postinstall is a tarball archive.
It contains many files AND a <tt>main</tt> scripts.
Postinstall contains actions that can't be put in the preinstall scripts.
Manage the ssh services, and the ldap what you think important for a node...


<!-- =========================== -->
<sect1>Cluster BootStrap
<p>
We will install a node named cls2, with ip 192.168.0.2.
You need a tarball of a linux box.
This tarball need a ssh server and a user deploy.
something like <tt>myLinux.tgz</tt> that can be created simply like this:
<verb>
#cd /
#tar czvlf /pathToAnotherFileSystem/myLinux.tgz /
</verb> 
<p>
You now need to record this environement in kadeploy database.
<verb>
#karecordenv 
            -n "myLinux" 
            -v 2 
            -d "Newbie testing" 
            -a noobs@mycluster.net 
            -fb file://home/noobs/myLinux.tgz 
            -ft file://home/noobs/traitement.tgz  
            -s 900 
            -i /boot/initrd.img-2.6.8-1-686-smp 
            -k /boot/vmlinuz-2.6.8-1-686-smp
</verb>

<p>
Now you got, an environnement, you have to watch for a preinstall, postinstall archive.
An example is given in <tt>lib/pre_post_script</tt> in the "$prefix" directory.

<p>
The cluster need a reference environnement to boot up properly.
You can boot up the deployment system, if your node isn't installed.
A simple solution, is 
<itemize>
<item>Set the nodes in deployement state with the <tt>setup_pxe.pl</tt> tools (grep label in deploy.conf).
 <verb>#setup_pxe.pl 192.168.0.2:label_deploy_x86 </verb>
<item>reboot the node whith what you want.
<item>When the node is reachable with ping and <tt>rsh -l root cls2 ls</tt> you can bootstrap the node kadeploy.
 <verb>kadeploy -e myLinux -p hda2 -m cls2</verb>
</itemize>

<p>
When this step is succefully done, you can ssh the node, and kadeploy a new environnement.
Your training is completed.




<!-- =========================== -->
<sect1>Other tools for remote management
<p>
Two remote management tools are available to diagnostic and if possible take control of cluster nodes that would be for instance in an unstable or undefined state further to a deployment failure. 
<p>
The <ref id="kaconsole" name="kaconsole"> tool enables to open a console on a remote node. 
It needs special hardware equipment and special command configuration. 
<p>
The <ref id="kareboot" name="kareboot"> tool enables to do various reboot types on cluster nodes. 
The possibilities are numerous : the reboot on a given (already installed) environment or 
on a given partition or 
even on the deployment kernel for instance. 
It also enables to hard reboot nodes appropriately equiped.
<!-- =========================== -->

<sect1>What if the cluster hardware composition changes ?
<p>
<ref id="kaaddnode" name="kaaddnode"> and <ref id="kadelnode" name="kadelnode"> enable to add and remove nodes from the deployment system if the cluster composition changes.
<!-- =========================== -->
<sect1>Command summary 
<p>
<sect2>Functionalities summary
<p><tt>kanodes</tt> - registers nodes in deployment system
<p><tt>karecordenv</tt> - registers an environment image in deployment system
<p><tt>kaenvironments</tt> - list existing registered environment.
<p><tt>kadeploy</tt> - deploys an environment image
<p><tt>kaconsole</tt> - opens a remote console 
<p><tt>kareboot</tt> - reboots according to requested reboot type
<p><tt>kadatabase</tt> - manage mysql schema.
<p>

<sect2>Use summary
<p>The first time...
<itemize>
  <item<tt>karecordenv</tt> - to register environments already installed (if any)
  <item><tt>kanodes</tt> - to register the cluster hardware composition
</itemize>
<p>Deployment
<itemize>
  <item><tt>karecordenv</tt> - to create and/or register an environment and make it available for deployment
  <item><tt>kadeploy</tt> - to deploy a registered environment
</itemize>
<p>Other tools
<itemize>
  <item><tt>kaconsole</tt> - to open a console on a remote node
  <item><tt>kareboot</tt> - to reboot a cluster node
</itemize>
<sect2>Examples
<p>
<verb>
# karecordenv -n debian -d "custom debian" -fb file://home/toto/images/custom_debian.tgz -ft file://home/toto/images/debian_postinstall.tgz -size 750 -k /boot/vmlinuz 
# kaanode -add
# karecordenv -e new_debian -fb file://home/toto/images/new_debian.tgz -ft file://home/toto/images/debian_postinstall.tgz --size 650 -k /boot/vmlinuz
# kadeploy -e new_debian -m node1 -m node2 -p hda7
# kaconsole -m node2
# kareboot -s -e custom_debian -m node2
</verb>

<!-- =========================== -->
<sect>About the customization scripts<label id="About_the_customization_scripts">
<p>
Kadeploy allows the customization of each node by 2 means:
<itemize>
  <item>preinstallation script, executed before sending the system image
  <item>postinstallation, executed after having sent the system image
</itemize>
Originally, these two scripts are written in <em>ash</em>, which is a lightweight bash, but the way these scripts are designed could allow to add any script language.

<!-- =========================== -->
<sect1>Preinstallation script
<p>
This script is common to all environments, its goal is to prepare the system to the hardware specification and the target hard disk drive for the deployment. It can load a specific IDE controller driver, improve deployment performance or make every kind of checks needed. This script is defined in the configuration file as <ref id="pre_install_script" name="pre_install_script"> and the associated archive as <ref id="pre_install_archive" name="pre_install_archive">.
<!-- =========================== -->
<sect2>Preinstallation archive structure
<p>
The preinstallation archive is a gzipped tar archive, containing the <ref id="pre_install_script" name="pre_install_script"> in its root directory.
Here is an example of a preinstallation archive structure:
<verb>
/
   init.ash
   lib/
   bin/
      awk
      df
      du
      xargs
</verb>

The directory structure allows to custom the tasks to your needs. In this example, the pre_install_script is <em>init.ash</em>. Let's have a short look at this structure :
<itemize>
   <item><verb>init.ash</verb> is my pre_install_script, so it needs to be there
   <item><verb>bin/</verb> is a directory where you can add custom binaries, here, I needed <em>awk</em>, <em>df</em>, <em>du</em> and <em>xargs</em>.
   <item><verb>lib/</verb> is a directory where you can add custom libraries for your binaries. I suggest you to compile static binaries, to prevent conflicts/version problems due to the presence of the system's shared libraries.
</itemize>

<!-- =========================== -->
<sect1>Postinstallation script
<p>
This script is associated to the environment to deploy. Its goal is to adapt the crude system image to a bootable system. It is composed of a gunzipped tar archive that contains all the sites files and a script <em>traitement.ash</em> in the archive's root directory. This archive is sent to the nodes, decompressed in a ramdisk and then the <tt>post_install_script</tt> is executed on every node. The script name is defined in the configuration file as <ref id="post_install_script" name="post_install_script">.
<!-- =========================== -->
<sect2>Postinstallation archive structure
<p>
The postinstallation archive is a gzipped tar archive, containing the <ref id="post_install_script" name="post_install_script"> in its root directory.
Here is an example of a postinstallation archive structure:
<verb>
/
   traitement.ash
   etc/
      fstab
      hosts
      hosts.allow
      hosts.deny
      ntpdate
   authorized_keys
</verb>

The directory structure allows to custom the configuration script to your needs. Let's have a short look at this structure :
<itemize>
   <item><verb>traitement.ash</verb> is my post_install_script.
   <item><verb>etc/</verb> is a directory where I decided to put all the files I wanted to replace on my system, this is an arbitrary choice, but allows to keep a clean structure.
   <item><verb>authorized_keys</verb> is the root's authorized_keys file I decided to put it in the root directory to be sure to have a look at it everytime I update my postinstallation archive.
</itemize>

<!-- =========================== -->
<sect2>System modifications
<p>
<enum>
   <item><verb>/etc/fstab</verb> a site base file should be copied in the postinstall archive so that nfs mounts can be preserved and other site modifications could be preserved
   <item><verb>/tmp</verb> should have its rights modified
</enum>
<!-- =========================== -->
<sect2>Administrative aspects
<p>
Many other basic files can be handled by putting them in the archive and replace the existing ones. 
They are not all listed here but only the most important ones :
<enum>
   <item><verb>/root/.ssh/authorized_keys</verb> should contain the administrator's public key and also the public key of user <tt>deploy</tt>, 
to allow him to get a root shell on every node to reboot those. 
In order to do that this authorized_keys file has to be built and put in the archive's root directory
   <item><verb>/etc/hosts /etc/hosts.allow /etc/hosts.deny</verb> should be set to fit the cluster's configuration, 
and ensure network connection within the cluster's network
</enum>

Numerous modifications can be done here, from authentification server to tailored modification depending on the node's IP. 
A good idea should be to modify <tt>rc</tt> scripts to prevent the first boot hard disk drive verification, 
because it is just a waste of time here, and avoid all the manual intervention that could occur on system boot : 
for example, by default, many distributions ask the root password before checking a filesystem on boot time.


<!-- =========================== -->
<sect>Example of Complete Configuration for Kadeploy 2.1.6
<p>
<em>kadeploy</em> tools suite is configured through two configuration files : <tt>deploy.conf</tt> and <tt>deploy_cmd.conf</tt> in the <tt>/etc/kadeploy/</tt> folder.
See the manpage for more.
<!-- =========================== -->
<!-- =========================== -->

  </article>
